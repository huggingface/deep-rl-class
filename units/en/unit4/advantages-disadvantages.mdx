# The advantages and disadvantages of Policy-based methods

At this point, you might ask "But Deep Q-Learning is excellent! Why use policy gradient methods?", let's study then the advantages and disadvantages of Policy-gradient methods

## Advantages

There are multiple advantages over Value-based methods. Let's see some of them:

### The simplicity of integration

Indeed, **we can estimate the policy directly without storing additional data (action values).**

### Policy-gradient methods can learn a stochastic policy

Policy gradient methods canÂ **learn a stochastic policy while value functions can't**.

This has two consequences:

1. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent exploresÂ **the state space without always taking the same trajectory.**

2. We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing is when two states seem (or are) the same but need different actions.

Let's take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg" alt="Hamster 1"/>
</figure>

Our vacuum cleaner can only perceive where the walls are.

The problem is that the two red cases are aliased states because the agent perceives an upper and lower wall for each.

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg" alt="Hamster 1"/>
</figure>

Under a deterministic policy, the policy will either move right when in a red state or move left. **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based RL algorithm, we learn a quasi-deterministic policy ("greedy epsilon strategy"). Consequently, our agent can spend a lot of time before finding the dust.

On the other hand, an optimal stochastic policy will randomly move left or right in grey states. Consequently, **it will not be stuck and will reach the goal state with a high probability**.

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg" alt="Hamster 1"/>
</figure>

### Policy-gradient methods are more effective in in high-dimensional action spaces and continuous actions spaces

Indeed, the problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.

But what if we have an infinite possibility of actions?

For instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15Â°, 17.2Â°, 19,4Â°, honking, etc.). We'll need to output a Q-value for each possible action! And taking the max action of a continuous output is an optimization problem itself!

Instead, with a policy gradient, we output aÂ **probability distribution over actions.**

### Policy-gradient methods have better convergence properties
In Value-based methods, we use an aggressive operator to **change the value function: we take the maximum over Q-estimates**.
Consequently, the action probabilities may change dramatically for an arbitrary small change in the estimated action-values if that change results in a different action having the maximal value.
For instance if during the training the best action was left (with Q-value of 0.22) and the training step after it's right (since the right Q-value become 0.23) we dramatically changed the policy since now the policy will take most of the time right instead of left.

On the other hand, in Policy-based methods, stochastic policy action preferences (probability of taking action) **changes smoothly over time**.

## Disadvantages

Naturally, Policy Gradient methods have also some disadvantages:

- **Policy gradients converge a lot of time on a local maximum instead of a global optimum.**
- Policy gradient goes less faster,Â **step by step: it can take longer to train (inefficient).**
- Policy gradient can have high variance, we'll see in Actor Critic unit why and how we can solve this problem.

ðŸ‘‰ If you want to go deeper on the advantages and disadvantages of Policy Gradients methods, [you can check this video](https://youtu.be/y3oqOjHilio).
