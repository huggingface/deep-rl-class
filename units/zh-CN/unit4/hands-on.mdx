# åŠ¨æ‰‹å°è¯•



      <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
      notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb"}
        ]}
        askForHelpUrl="http://hf.co/join/discord" />



ç°åœ¨æˆ‘ä»¬å·²ç»å­¦ä¹ äº†å¼ºåŒ–ç®—æ³•èƒŒåçš„ç†è®ºï¼Œ**ä½ å·²ç»å‡†å¤‡å¥½ç”¨ Pytorch ç¼–å†™ä½ çš„å¼ºåŒ–ç®—æ³•æ™ºèƒ½ä½“äº†**ã€‚å¹¶ä¸”ä½ ä¼šç”¨ CartPole-v1 å’Œ PixelCopter æ¥æµ‹è¯•ä»–çš„é²æ£’æ€§ã€‚

ä½ å¯ä»¥é€šè¿‡æ›´é«˜çº§çš„ç¯å¢ƒè¿­ä»£å’Œæå‡è¿™ä¸ªæ“ä½œã€‚

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>
</figure>

ä¸ºäº†é€šè¿‡è®¤è¯è¿‡ç¨‹ä¸­çš„å®è·µéªŒè¯ï¼Œä½ éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ° Hub ä¸Šã€‚

- `Cartpole-v1` çš„ç»“æœ >= 350ã€‚
- `PixelCopter` çš„ç»“æœ >= 5ã€‚

è¦æŸ¥çœ‹ä½ çš„ç»“æœï¼Œè¯·è½¬åˆ° leaderboard å¹¶æ‰¾åˆ°ä½ çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**ã€‚**å¦‚æœåœ¨ leaderboard ä¸Šæ‰¾ä¸åˆ°ä½ çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°é¡µé¢åº•éƒ¨å¹¶å•å‡»åˆ·æ–°æŒ‰é’®ã€‚**

**å¦‚æœä½ æ‰¾ä¸åˆ°ä½ çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°é¡µé¢åº•éƒ¨å¹¶å•å‡»åˆ·æ–°æŒ‰é’®ã€‚**

æ›´å¤šå…³äºè®¤è¯è¿‡ç¨‹ï¼Œç‚¹å‡»è¿™ä¸€éƒ¨åˆ† ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

å¹¶ä¸”ä½ å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥ä½ çš„è¿‡ç¨‹ ğŸ‘‰ https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course


**ç‚¹å‡»æ‰“å¼€ä¸‹æ–¹æŒ‰é”®å¼€å§‹** ğŸ‘‡ :

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)


# å•å…ƒ 4: ç”¨ Pytorch ç¼–å†™ä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šå¼ºåŒ–ç®—æ³•ï¼Œå¹¶æµ‹è¯•ä»–çš„é²æ£’æ€§ ğŸ’ª

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png" alt="thumbnail"/>


åœ¨æœ¬ notebook ä¸­, ä½ å°†ä»å¤´å¼€å§‹ç¼–å†™ä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šå¼ºåŒ–ç®—æ³•ï¼ˆä¹Ÿç§°ä¸ºè’™ç‰¹å¡ç½—ç­–ç•¥æ¢¯åº¦ï¼‰ã€‚

å¼ºåŒ–ç®—æ³•æ˜¯ä¸€ç§*åŸºäºç­–ç•¥çš„æ–¹æ³•*ï¼Œä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯•å›¾ç›´æ¥ä¼˜åŒ–ç­–ç•¥è€Œä¸ä½¿ç”¨åŠ¨ä½œå€¼å‡½æ•°ã€‚

æ›´ç²¾ç¡®åœ°è¯´ï¼Œå¼ºåŒ–ç®—æ³•æ˜¯*åŸºäºç­–ç•¥çš„æ¢¯åº¦æ–¹æ³•*ï¼Œæ˜¯*åŸºäºç­–ç•¥çš„æ–¹æ³•*çš„ä¸€ä¸ªå­ç±»ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ä¼°è®¡æœ€ä¼˜ç­–ç•¥çš„æƒé‡æ¥ç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚

ä¸ºäº†æµ‹è¯•å…¶é²æ£’æ€§ï¼Œæˆ‘ä»¬å°†åœ¨ä¸¤ä¸ªä¸åŒçš„ç®€å•ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼š
- Cartpole-v1
- PixelcopterEnv

â¬‡ï¸ ä»¥ä¸‹æ˜¯æ­¤ notebook ç»“æŸæ—¶ä½ å°†èƒ½å¤Ÿå®ç°çš„ç¤ºä¾‹ã€‚â¬‡ï¸

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>


### ğŸ® ç¯å¢ƒï¼š

- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)
- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)

### ğŸ“š RL-åº“:

- Python
- PyTorch


æˆ‘ä»¬ä¼šæŒç»­æå‡æˆ‘ä»¬çš„æ•™ç¨‹è´¨é‡ï¼Œå¦‚æœä½ åœ¨ notebook ä¸­å‘ç°äº†ä»€ä¹ˆé—®é¢˜è¯· [åœ¨ Github Repo ä¸Šæå‡º issue](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## ç›®æ ‡ ğŸ†

åœ¨ notebook ç»“æŸï¼Œä½ å°†ä¼šå¾—åˆ°ï¼š

- èƒ½å¤Ÿ**ä½¿ç”¨ PyTorch ä»å¤´å¼€å§‹ç¼–å†™å¼ºåŒ–ç®—æ³•**ã€‚
- èƒ½å¤Ÿä½¿ç”¨**ç®€å•çš„ç¯å¢ƒæµ‹è¯•ä½ çš„æ™ºèƒ½ä½“çš„é²æ£’æ€§**ã€‚
- èƒ½å¤Ÿå°†**ä½ è®­ç»ƒçš„æ™ºèƒ½ä½“æ¨é€åˆ° Hub**ï¼Œå¹¶æä¾›ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°å¾—åˆ†ã€‚ğŸ”¥

## å‰ç½®å‡†å¤‡ ğŸ—ï¸

åœ¨è¿›å…¥ notebook å‰ï¼Œä½ éœ€è¦ï¼š

ğŸ”² ğŸ“š [å­¦ä¹ ç¬¬ 4 å•å…ƒçš„ç­–ç•¥æ¢¯åº¦](https://huggingface.co/deep-rl-course/unit4/introduction)

# è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ç¼–å†™ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³• ğŸ”¥

## å»ºè®® ğŸ’¡

æœ€å¥½å°†è¿™ä¸ª colab å¤åˆ¶åˆ°ä½ çš„ Google Drive ä¸­è¿è¡Œï¼Œè¿™æ ·å¦‚æœ**å®ƒè¶…æ—¶äº†**ï¼Œä½ ä»ç„¶å¯ä»¥åœ¨ä½ çš„ Google Drive ä¸­ä¿å­˜å·²ä¿å­˜çš„ç¬”è®°æœ¬ï¼Œè€Œä¸éœ€è¦é‡æ–°å¼€å§‹ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨ `Ctrl + S` æˆ– `File > Save a copy in Google Drive`ã€‚

## è®¾ç½® GPU ğŸ’ª

- ä¸ºäº† **åŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ GPU**. ç‚¹å‡» `Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1">

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2">

## åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ˜¾ç¤º ğŸ–¥

åœ¨ Notebook ä¸­, æˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªå›æ”¾è§†é¢‘ã€‚åœ¨ colab ä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦ä¸€ä¸ªè™šæ‹Ÿå±å¹•å»æ¸²æŸ“ç¯å¢ƒ**ï¼ˆå¹¶è®°å½•å¸§ï¼‰ã€‚

æ‰€ä»¥å‚ç…§ä¸‹é¢å•å…ƒæ ¼å®‰è£…ç›¸å…³åº“å¹¶è¿è¡Œè™šæ‹Ÿå±å¹• ğŸ–¥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay
!pip install pyglet==1.5.1
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## å®‰è£…ä¾èµ– ğŸ”½

ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–ï¼Œæˆ‘ä»¬å°†ä¼šå®‰è£…å¤šä¸ªä¾èµ–ï¼š

- `gym`
- `gym-games`: ä½¿ç”¨ PyGame åˆ¶ä½œçš„é¢å¤– gym ç¯å¢ƒã€‚
- `huggingface_hub`: Hub æ˜¯ä¸€ä¸ªä¸­å¿ƒåŒ–çš„å¹³å°ï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨å…¶ä¸­åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒæä¾›ç‰ˆæœ¬æ§åˆ¶ã€åº¦é‡ã€å¯è§†åŒ–ç­‰åŠŸèƒ½ï¼Œä½¿ä½ èƒ½å¤Ÿè½»æ¾åœ°ä¸ä»–äººåˆä½œã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ‰€æœ‰çš„å¯ç”¨çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ ğŸ‘‰ https://huggingface.co/models?other=reinforce

å¹¶ä¸”ä½ åœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°æ‰€æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ ğŸ‘‰ https://huggingface.co/models?pipeline_tag=reinforcement-learning


```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt
```

## å¯¼å…¥åŒ… ğŸ“¦

é™¤äº†å¯¼å…¥å®‰è£…è¿‡çš„åº“ï¼Œæˆ‘ä»¬åŒæ ·è¦å¯¼å…¥ï¼š

- `imageio`: ä¸€ä¸ªå¸®åŠ©æˆ‘ä»¬ç”Ÿæˆè§†é¢‘å›æ”¾çš„åº“



```python
import numpy as np

from collections import deque

import matplotlib.pyplot as plt
%matplotlib inline

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Gym
import gym
import gym_pygame

# Hugging Face Hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
import imageio
```

## æ£€æŸ¥ä¸€ä¸‹ GPU

- è®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬æœ‰æ²¡æœ‰ GPU
- å¦‚æœæœ‰åˆ™å¯ä»¥çœ‹è§ `device:cuda0`

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

```python
print(device)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¼ºåŒ–å­¦ä¹ çš„ç®—æ³•äº† ğŸ”¥

# ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“ï¼šä½¿ç”¨ CartPole-v1 ğŸ¤–

## åˆ›å»º CartPole ç¯å¢ƒå¹¶ç†è§£å®ƒæ€ä¹ˆè¿è¡Œçš„

### [ç¯å¢ƒ ğŸ®](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)

### ä¸ºä»€ä¹ˆä½¿ç”¨åƒ CartPole-v1 è¿™æ ·ç®€å•çš„ç¯å¢ƒ?

å¦‚ [å¼ºåŒ–å­¦ä¹ æŠ€å·§å’ŒæŠ€å·§](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html) ä¸­æ‰€è¿°ï¼Œå½“ä½ ä»å¤´å¼€å§‹å®ç°ä½ çš„æ™ºèƒ½ä½“æ—¶ï¼Œä½ éœ€è¦ç¡®ä¿å®ƒ**åœ¨ç®€å•ç¯å¢ƒä¸­å¯ä»¥æ­£ç¡®å·¥ä½œå¹¶æ‰¾åˆ°æ˜“äºå‡ºé”™çš„åœ°æ–¹ï¼Œç„¶åå†æ·±å…¥è¿›è¡Œ**ã€‚å› ä¸ºåœ¨ç®€å•ç¯å¢ƒä¸­å‘ç°é”™è¯¯å°†æ›´å®¹æ˜“ã€‚

> å°è¯•åœ¨ç©å…·é—®é¢˜ä¸Šæœ‰ä¸€äº›â€œç”Ÿå‘½çš„è¿¹è±¡â€

> é€šè¿‡åœ¨è¶Šæ¥è¶Šå›°éš¾çš„ç¯å¢ƒä¸­è¿è¡Œæ¥éªŒè¯å®ç°ï¼ˆå¯ä»¥å°†ç»“æœä¸ RL zoo è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚ä½ é€šå¸¸éœ€è¦è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–æ‰èƒ½è¿›è¡Œè¿™ä¸€æ­¥ã€‚

### CartPole-v1ç¯å¢ƒ

> ä¸€ä¸ªæ†é€šè¿‡ä¸€ä¸ªæœªæ¿€æ´»çš„å…³èŠ‚è¿æ¥åˆ°ä¸€ä¸ªå°è½¦ä¸Šï¼Œå°è½¦æ²¿ç€æ— æ‘©æ“¦è½¨é“ç§»åŠ¨ã€‚å°†æŒ‚é’©ç½®äºè½¦ä¸Šå¹¶é€šè¿‡åœ¨è½¦çš„å·¦å³æ–¹å‘ä¸Šæ–½åŠ åŠ›æ¥ä¿æŒæ†çš„å¹³è¡¡ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬ä» CartPole-v1 å¼€å§‹ã€‚ç›®æ ‡æ˜¯å‘å·¦æˆ–å‘å³æ¨è½¦ï¼Œ**ä»¥ä¾¿æ†ä¿æŒå¹³è¡¡**ã€‚

å¦‚æœä»¥ä¸‹ä»»ä¸€æ¡ä»¶æˆç«‹ï¼Œæœ¬è½®ç‰‡æ®µå°±ä¼šç»“æŸï¼š
- æ†çš„è§’åº¦å¤§äº Â±12Â°
- è½¦çš„ä½ç½®å¤§äº Â±2.4
- æœ¬è½®ç‰‡æ®µçš„æŒç»­æ—¶é—´å¤§äº 500

æ¯ä¸ªæ—¶é—´æ­¥ï¼Œå¦‚æœæ†ä¿æŒåœ¨å¹³è¡¡çŠ¶æ€ï¼Œåˆ™æˆ‘ä»¬è·å¾— +1 çš„å¥–åŠ±ğŸ’°ã€‚


```python
env_id = "CartPole-v1"
# Create the env
env = gym.make(env_id)

# Create the evaluation env
eval_env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

## è®©æˆ‘ä»¬å»ºç«‹å¼ºåŒ–ç®—æ³•ç»“æ„

è¿™ä¸ªè¿‡ç¨‹åŸºäºä»¥ä¸‹ä¸‰ä¸ªè¿‡ç¨‹ï¼šThis implementation is based on three implementations:
- [PyTorch å®˜æ–¹å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)
- [Udacity å¼ºåŒ–ç®—æ³•](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)
- [Chris1nexus æ”¹è¿›çš„é›†æˆ](https://github.com/huggingface/deep-rl-class/pull/95)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png" alt="Reinforce"/>

æ‰€ä»¥æˆ‘ä»¬æƒ³è¦ï¼š
- ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆfc1 å’Œ fc2ï¼‰ã€‚
- ä½¿ç”¨ ReLU ä½œä¸º fc1 çš„æ¿€æ´»å‡½æ•°
- ä½¿ç”¨ Softmax æ¥è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Create two fully connected layers



    def forward(self, x):
        # Define the forward pass
        # state goes to fc1 then we apply ReLU activation function

        # fc1 outputs goes to fc2

        # We output the softmax

    def act(self, state):
        """
        Given a state, take action
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

### å‚è€ƒç­”æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œä½ èƒ½æ‰¾åˆ°ä»–åœ¨é‚£é‡Œå—ï¼Ÿ

- ä¸ºäº†æ‰¾åˆ°é”™è¯¯ï¼Œè®©æˆ‘ä»¬è¿›è¡Œä¸€ä¸‹å‰å‘ä¼ æ’­ï¼š

```python
debug_policy = Policy(s_size, a_size, 64).to(device)
debug_policy.act(env.reset())
```

- è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°é”™è¯¯æ˜¾ç¤ºä¸º `ValueError: The value argument to log_prob must be a Tensor`

- è¿™æ„å‘³ç€ `m.log_prob(action)` ä¸­çš„ `action` å¿…é¡»æ˜¯ä¸€ä¸ª Tensor **ä½†å®ƒä¸æ˜¯ã€‚**

- ä½ çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿæ£€æŸ¥ act å‡½æ•°å¹¶å°è¯•æ‰¾å‡ºåŸå› ã€‚

å»ºè®® ğŸ’¡: è¿™ä¸ªå®ç°ä¸­æœ‰äº›é—®é¢˜ã€‚è®°ä½ï¼Œæˆ‘ä»¬åœ¨ act å‡½æ•°ä¸­ **æƒ³è¦ä»åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œã€‚**


### (çœŸ) å‚è€ƒç­”æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

é€šè¿‡ä½¿ç”¨ CartPole ï¼Œæˆ‘ä»¬æ›´å®¹æ˜“è°ƒè¯•ï¼Œå› ä¸º**æˆ‘ä»¬çŸ¥é“é”™è¯¯æ¥è‡ªäºæˆ‘ä»¬çš„é›†æˆï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ç®€å•çš„ç¯å¢ƒ**ã€‚

- ç”±äº**æˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡ç”¨ä¸€ä¸ªåŠ¨ä½œ**ï¼Œæˆ‘ä»¬å°±ä¸èƒ½ä½¿ç”¨  `action = np.argmax(m)` å› ä¸ºä»–æ€»ä¼šè¾“å‡ºæ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œã€‚

- æˆ‘ä»¬éœ€è¦ä»æ¦‚ç‡åˆ†å¸ƒ P(.|s) é‡‡æ ·æ¥æ›¿ä»£ `action = m.sample()` ã€‚

### è®©æˆ‘ä»¬æ„å»ºå¼ºåŒ–è®­ç»ƒç®—æ³•
ä¸‹é¢æ˜¯å¼ºåŒ–ç®—æ³•çš„ä¼ªä»£ç ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png" alt="Policy gradient pseudocode"/>


- å½“æˆ‘ä»¬è®¡ç®—å›æŠ¥ Gtï¼ˆç¬¬ 6 è¡Œï¼‰æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è®¡ç®—äº†**ä»æ—¶é—´æ­¥ t å¼€å§‹çš„æŠ˜æ‰£å¥–åŠ±æ€»å’Œ**ã€‚

- ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæˆ‘ä»¬çš„ç­–ç•¥åº”è¯¥åªåŸºäºåæœå¼ºåŒ–è¡ŒåŠ¨ï¼šå› æ­¤ï¼Œåœ¨æ‰§è¡Œè¡ŒåŠ¨ä¹‹å‰è·å¾—çš„å¥–åŠ±æ˜¯æ— ç”¨çš„ï¼ˆå› ä¸ºå®ƒä»¬ä¸æ˜¯å› ä¸ºè¡ŒåŠ¨è€Œè·å¾—çš„ï¼‰ï¼Œ**åªæœ‰åœ¨è¡ŒåŠ¨ä¹‹åè·å¾—çš„å¥–åŠ±æ‰æ˜¯æœ‰æ„ä¹‰çš„**ã€‚

- åœ¨ç¼–ç ä¹‹å‰ï¼Œä½ åº”è¯¥é˜…è¯»æœ¬èŠ‚[ä¸è¦è®©è¿‡å»åˆ†æ•£ä½ çš„æ³¨æ„åŠ›](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)ï¼Œå…¶ä¸­è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨å›æŠ¥é€æ­¥ç­–ç•¥æ¢¯åº¦ã€‚

æˆ‘ä»¬ä½¿ç”¨ç”± [Chris1nexus](https://github.com/Chris1nexus) ç¼–å†™çš„ä¸€ç§æœ‰è¶£çš„æŠ€æœ¯æ¥**é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥**ã€‚æ³¨é‡Šè§£é‡Šäº†è¯¥è¿‡ç¨‹ã€‚è¿˜å¯ä»¥[æ£€æŸ¥ PR è§£é‡Š](https://github.com/huggingface/deep-rl-class/pull/95)
ä½†æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æƒ³æ³•æ˜¯**é«˜æ•ˆåœ°è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥**ã€‚

ç¬¬äºŒä¸ªé—®é¢˜æ˜¯ï¼Œä¸ºä»€ä¹ˆè¦**æœ€å°åŒ–æŸå¤±**ï¼Ÿæˆ‘ä»¬ä¸æ˜¯åœ¨è®²æ¢¯åº¦ä¸Šå‡å—ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™å—ï¼Ÿ

- æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æ•ˆç”¨å‡½æ•° $J(\theta)$ ï¼Œä½†åœ¨ PyTorch å’Œ TensorFlow ä¸­ï¼Œæœ€å¥½**æœ€å°åŒ–ç›®æ ‡å‡½æ•°**ã€‚
    - å‡è®¾æˆ‘ä»¬æƒ³åœ¨æŸä¸ªæ—¶é—´æ­¥å¼ºåŒ–åŠ¨ä½œ 3ã€‚åœ¨è®­ç»ƒæ­¤åŠ¨ä½œ P ä¹‹å‰ï¼Œä¸º 0.25ã€‚
    - æ‰€ä»¥æˆ‘ä»¬æƒ³ä¿®æ”¹ \\(theta\\)ï¼Œä½¿å¾— \\(\pi_\theta(a_3|s; \theta) > 0.25\\)
    - å› ä¸ºæ‰€æœ‰ P å¿…é¡»æ€»å’Œä¸º 1ï¼Œmax \\(pi_\theta(a_3|s; \theta)\\) å°†**å‡å°å…¶ä»–åŠ¨ä½œæ¦‚ç‡ã€‚**
    - å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥å‘Šè¯‰ PyTorch **æœ€å°åŒ– \\(1-\pi_\theta(a_3|s; \theta)\\)ã€‚**
    - æ­¤æŸå¤±å‡½æ•°åœ¨ \\(\pi_\theta(a_3|s; \theta)\\) æ¥è¿‘ 1 æ—¶è¶‹è¿‘äº 0ã€‚
    - å› æ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±æ¢¯åº¦æœ€å¤§åŒ– \\(\pi_\theta(a_3|s; \theta)\\)

```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes+1):
        saved_log_probs = []
        rewards = []
        state = # TODO: reset the environment
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = # TODO get the action
            saved_log_probs.append(log_prob)
            state, reward, done, _ = # TODO: take an env step
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t

        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...


        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = (returns[0] if len(returns)>0 else 0)
            returns.appendleft(    ) # TODO: complete here

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()

        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))

    return scores
```

#### å‚è€ƒç­”æ¡ˆ

```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes + 1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as
        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t
        #
        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...

        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = returns[0] if len(returns) > 0 else 0
            returns.appendleft(gamma * disc_return_t + rewards[t])

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()
        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print("Episode {}\tAverage Score: {:.2f}".format(i_episode, np.mean(scores_deque)))

    return scores
```

##  è®­ç»ƒä»–
- æˆ‘ä»¬ç°åœ¨å·²ç»å‡†å¤‡å¥½è®­ç»ƒæ™ºèƒ½ä½“äº†ã€‚
- é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå˜é‡åŒ…å«æ‰€æœ‰è¶…å‚æ•°ã€‚
- ä½ å¯ä»¥æ›´æ”¹è®­ç»ƒå‚æ•°ï¼ˆåº”è¯¥æ›´æ”¹ ğŸ˜‰ï¼‰

```python
cartpole_hyperparameters = {
    "h_size": 16,
    "n_training_episodes": 1000,
    "n_evaluation_episodes": 10,
    "max_t": 1000,
    "gamma": 1.0,
    "lr": 1e-2,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

```python
# Create policy and place it to the device
cartpole_policy = Policy(
    cartpole_hyperparameters["state_space"],
    cartpole_hyperparameters["action_space"],
    cartpole_hyperparameters["h_size"],
).to(device)
cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters["lr"])
```

```python
scores = reinforce(
    cartpole_policy,
    cartpole_optimizer,
    cartpole_hyperparameters["n_training_episodes"],
    cartpole_hyperparameters["max_t"],
    cartpole_hyperparameters["gamma"],
    100,
)
```

## å®šä¹‰è¯„ä¼°æ–¹æ³• ğŸ“
- è¿™é‡Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬å°†è¦æµ‹è¯•æˆ‘ä»¬çš„å¼ºåŒ–æ™ºèƒ½ä½“çš„è¯„ä¼°æ–¹æ³•ã€‚

```python
def evaluate_agent(env, max_steps, n_eval_episodes, policy):
    """
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param policy: The Reinforce agent
    """
    episode_rewards = []
    for episode in range(n_eval_episodes):
        state = env.reset()
        step = 0
        done = False
        total_rewards_ep = 0

        for step in range(max_steps):
            action, _ = policy.act(state)
            new_state, reward, done, info = env.step(action)
            total_rewards_ep += reward

            if done:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## è¯„ä¼°æˆ‘ä»¬çš„æ™ºèƒ½ä½“ ğŸ“ˆ

```python
evaluate_agent(
    eval_env, cartpole_hyperparameters["max_t"], cartpole_hyperparameters["n_evaluation_episodes"], cartpole_policy
)
```

### å°†æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹å‘å¸ƒåˆ° Hub ä¸Š ğŸ”¥
ç°åœ¨æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬é€šè¿‡è®­ç»ƒå¾—åˆ°äº†éå¸¸å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç æŠŠæˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ° hub ğŸ¤— ä¸Šã€‚

è¿™é‡Œæ˜¯æ¨¡å‹å¡çš„ä¾‹å­ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png"/>

### æ¨é€åˆ° hub
#### è¯·å‹¿ä¿®æ”¹æ­¤ä»£ç 

```python
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
import imageio

import tempfile

import os
```

```python
def record_video(env, policy, out_directory, fps=30):
    """
    Generate a replay video of the agent
    :param env
    :param Qtable: Qtable of our agent
    :param out_directory
    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
    """
    images = []
    done = False
    state = env.reset()
    img = env.render(mode="rgb_array")
    images.append(img)
    while not done:
        # Take the action (index) that have the maximum expected future reward given that state
        action, _ = policy.act(state)
        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic
        img = env.render(mode="rgb_array")
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```python
def push_to_hub(repo_id,
                model,
                hyperparameters,
                eval_env,
                video_fps=30
                ):
  """
  Evaluate, Generate a video and Upload a model to Hugging Face Hub.
  This method does the complete pipeline:
  - It evaluates the model
  - It generates the model card
  - It generates a replay video of the agent
  - It pushes everything to the Hub

  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
  :param model: the pytorch model we want to save
  :param hyperparameters: training hyperparameters
  :param eval_env: evaluation environment
  :param video_fps: how many frame per seconds to record our video replay
  """

  _, repo_name = repo_id.split("/")
  api = HfApi()

  # Step 1: Create the repo
  repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
  )

  with tempfile.TemporaryDirectory() as tmpdirname:
    local_directory = Path(tmpdirname)

    # Step 2: Save the model
    torch.save(model, local_directory / "model.pt")

    # Step 3: Save the hyperparameters to JSON
    with open(local_directory / "hyperparameters.json", "w") as outfile:
      json.dump(hyperparameters, outfile)

    # Step 4: Evaluate the model and build JSON
    mean_reward, std_reward = evaluate_agent(eval_env,
                                            hyperparameters["max_t"],
                                            hyperparameters["n_evaluation_episodes"],
                                            model)
    # Get datetime
    eval_datetime = datetime.datetime.now()
    eval_form_datetime = eval_datetime.isoformat()

    evaluate_data = {
          "env_id": hyperparameters["env_id"],
          "mean_reward": mean_reward,
          "n_evaluation_episodes": hyperparameters["n_evaluation_episodes"],
          "eval_datetime": eval_form_datetime,
    }

    # Write a JSON file
    with open(local_directory / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = hyperparameters["env_id"]

    metadata = {}
    metadata["tags"] = [
          env_name,
          "reinforce",
          "reinforcement-learning",
          "custom-implementation",
          "deep-rl-class"
      ]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
      )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Reinforce** Agent playing **{env_id}**
  This is a trained model of a **Reinforce** agent playing **{env_id}** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """

    readme_path = local_directory / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
          readme = f.read()
    else:
      readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
      f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path =  local_directory / "replay.mp4"
    record_video(env, model, video_path, video_fps)

    # Step 7. Push everything to the Hub
    api.upload_folder(
          repo_id=repo_id,
          folder_path=local_directory,
          path_in_repo=".",
    )

    print(f"Your model is pushed to the Hub. You can view your model here: {repo_url}")
```

é€šè¿‡ä½¿ç”¨ `push_to_hub` ï¼Œ**ä½ çš„è¯„ä¼°ï¼Œå›æ”¾è®°å½•ï¼Œå·²ç»ç”Ÿæˆçš„æ¨¡å‹å¡éƒ½å·²ç»æ¨é€åˆ° Hub ä¸Šäº†**ã€‚

è¿™é‡Œï¼š
- ä½ å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥
- ä½ å¯ä»¥**å¯è§†åŒ–æ’­æ”¾ä½ çš„æ™ºèƒ½ä½“** ğŸ‘€
- ä½ å¯ä»¥**å‘ç¤¾åŒºåˆ†äº«ä¸€ä¸ªåˆ«äººå¯ä»¥ç”¨çš„æ™ºèƒ½ä½“** ğŸ’¾
- ä½ å¯ä»¥**åœ¨ leaderboard ğŸ† ä¸Šçœ‹ä½ çš„æ™ºèƒ½ä½“çš„è¡¨ç°å¹¶ä¸åŒå­¦å¯¹æ¯”** ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard


ä¸ºäº†èƒ½å¤Ÿåˆ†äº«ä½ çš„æ¨¡å‹åˆ°ç¤¾åŒºï¼Œä½ éœ€è¦éµå¾ªä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ (å¦‚æœæ²¡æœ‰) åˆ›å»ºä¸€ä¸ª HF è´¦å· â¡ https://huggingface.co/join

2ï¸âƒ£ ç™»é™†, ä»ç½‘ç«™ä¸­è·å–è®¤è¯ tokenã€‚
- åˆ›å»ºä¸€ä¸ªå…·æœ‰**å†™æƒé™**çš„æ–° token (https://huggingface.co/settings/tokens)


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token">


```python
notebook_login()
```

å¦‚æœä½ ä¸æƒ³ä½¿ç”¨ Google Colab æˆ–è€… Jupyter Notebookï¼Œä½ éœ€è¦ç”¨è¿™è¡Œå‘½ä»¤æ›¿ä»£ï¼š`huggingface-cli login` (or `login`)

3ï¸âƒ£ æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç”¨ `package_to_hub()` å‡½æ•°æ¨é€æˆ‘ä»¬è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“åˆ° ğŸ¤— Hub ğŸ”¥ ä¸Šã€‚

```python
repo_id = ""  # TODO Define your repo id {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    cartpole_policy,  # The model we want to save
    cartpole_hyperparameters,  # Hyperparameters
    eval_env,  # Evaluation environment
    video_fps=30
)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ‰§è¡Œæµ‹è¯•é²æ£’æ€§ï¼Œè®©æˆ‘ä»¬è¯•ä¸€ä¸ªæ›´å¤æ‚çš„ç¯å¢ƒï¼š PixelCopter ğŸš




## ç¬¬äºŒä¸ªæ™ºèƒ½ä½“: PixelCopter ğŸš

### å­¦ä¹  PixelCopter ç¯å¢ƒ ğŸ‘€
- [ç¯å¢ƒæ‰‹å†Œ](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)


```python
env_id = "Pixelcopter-PLE-v0"
env = gym.make(env_id)
eval_env = gym.make(env_id)
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

è§‚æµ‹ç©ºé—´ (7) ğŸ‘€:
- ç©å®¶çš„yåæ ‡
- ç©å®¶é€Ÿåº¦
- ç©å®¶åˆ°åœ°é¢çš„è·ç¦»
- ç©å®¶åˆ°å¤©èŠ±æ¿çš„è·ç¦»
- ä¸‹ä¸€ä¸ªæ–¹å—åˆ°ç©å®¶çš„xè·ç¦»
- ä¸‹ä¸€ä¸ªæ–¹å—çš„ä¸Šéƒ¨yä½ç½®
- ä¸‹ä¸€ä¸ªæ–¹å—çš„ä¸‹éƒ¨yä½ç½®

åŠ¨ä½œç©ºé—´ (2) ğŸ®:
- ä¸Š
- ä¸‹

å¥–åŠ±å‡½æ•° ğŸ’°:
- æ¯ç©¿è¿‡ä¸€ä¸ªå‚ç›´çš„æ–¹å—ï¼Œå®ƒè·å¾—ä¸€ä¸ª +1 çš„æ­£å¥–åŠ±ã€‚æ¯å½“åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œå®ƒä¼šè·å¾—ä¸€ä¸ª -1 çš„è´Ÿå¥–åŠ±ã€‚

### å®šä¹‰æ–°ç­–ç•¥ ğŸ§ 
- å› ä¸ºç¯å¢ƒæ›´å¤æ‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æ·±çš„ç¥ç»ç½‘ç»œ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Define the three layers here

    def forward(self, x):
        # Define the forward process here
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

#### å‚è€ƒç­”æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, h_size * 2)
        self.fc3 = nn.Linear(h_size * 2, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

### å®šä¹‰è¶…å‚æ•° âš™ï¸
- å› ä¸ºç¯å¢ƒæ›´å¤æ‚.
- å°¤å…¶å¯¹äºéšè—å±‚å¤§å°ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ç¥ç»å…ƒ.

```python
pixelcopter_hyperparameters = {
    "h_size": 64,
    "n_training_episodes": 50000,
    "n_evaluation_episodes": 10,
    "max_t": 10000,
    "gamma": 0.99,
    "lr": 1e-4,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

###  è®­ç»ƒå®ƒ
- æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ™ºèƒ½ä½“äº† ğŸ”¥.

```python
# Create policy and place it to the device
# torch.manual_seed(50)
pixelcopter_policy = Policy(
    pixelcopter_hyperparameters["state_space"],
    pixelcopter_hyperparameters["action_space"],
    pixelcopter_hyperparameters["h_size"],
).to(device)
pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters["lr"])
```

```python
scores = reinforce(
    pixelcopter_policy,
    pixelcopter_optimizer,
    pixelcopter_hyperparameters["n_training_episodes"],
    pixelcopter_hyperparameters["max_t"],
    pixelcopter_hyperparameters["gamma"],
    1000,
)
```

### å°†æ¨¡å‹å‘å¸ƒåˆ° Hub ğŸ”¥

```python
repo_id = ""  # TODO Define your repo id {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    pixelcopter_policy,  # The model we want to save
    pixelcopter_hyperparameters,  # Hyperparameters
    eval_env,  # Evaluation environment
    video_fps=30
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†

æœ€å¥½çš„å­¦ä¹ æ–¹æ³•æ˜¯**è‡ªå·±å°è¯•**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„æ™ºèƒ½ä½“æ•ˆæœå¹¶ä¸å¥½ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œå¯ä»¥å°è¯•è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚ä½†ä¹Ÿå¯ä»¥å°è¯•æ‰¾åˆ°æ›´å¥½çš„å‚æ•°ã€‚

åœ¨ [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) ä¸Šä½ ä¼šæ‰¾åˆ°ä½ çš„æ™ºèƒ½ä½“ï¼Œä½ èƒ½æ‰“æ¦œç™»é¡¶å—ï¼Ÿ

è¿™é‡Œæœ‰ä¸€äº›æ–¹ä¾¿å¸®ä½ è¾¾åˆ°ï¼š
* è®­ç»ƒæ›´å¤šçš„æ­¥éª¤
* å°è¯•ä¸åŒçš„è¶…å‚æ•°çœ‹çœ‹ä½ çš„åŒä¼´å®Œæˆçš„ ğŸ‘‰ https://huggingface.co/models?other=reinforce
* **æŠŠæ–°çš„æ¨¡å‹æ¨é€åˆ°** Hub ğŸ”¥
* **ä½¿ç”¨æ›´å¤æ‚çš„ç¯å¢ƒè®­ç»ƒ** (ä¸¾ä¸ªä¾‹å­ï¼Œå°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œæ¥å¤„ç†è§‚æµ‹æ¡†æ¶)?

________________________________________________________________________

**æ­å–œä½ å®Œæˆäº†æœ¬å•å…ƒçš„å­¦ä¹ **ï¼è¿™æ˜¯éå¸¸å¤šä¿¡æ¯çš„ä¸€ä¸ªå•å…ƒã€‚ç¥è´ºæ‚¨æˆåŠŸåœ°ä½¿ç”¨ PyTorch ä»å¤´å¼€å§‹ç¼–å†™äº†ä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå¹¶åˆ†äº«åˆ°äº† Hub ğŸ¥³ã€‚

å¦‚æœä½ æƒ³è¿›ä¸€æ­¥**æ”¹è¿›å®ç°ä»¥å¤„ç†æ›´å¤æ‚çš„ç¯å¢ƒ**ï¼ˆä¾‹å¦‚å°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†è§‚å¯Ÿå¸§ï¼‰ï¼Œè¯·éšæ—¶è¿­ä»£æ­¤å•å…ƒã€‚

åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­ï¼Œ**æˆ‘ä»¬å°†å­¦ä¹ æ›´å¤šå…³äº Unity MLAgents çš„å†…å®¹**ï¼Œé€šè¿‡è®­ç»ƒåœ¨ Unity ç¯å¢ƒä¸­çš„æ™ºèƒ½ä½“ï¼Œä½ å°†å‡†å¤‡å¥½å‚åŠ  **AI vs AI æŒ‘æˆ˜ï¼Œè®­ç»ƒä½ çš„æ™ºèƒ½ä½“åœ¨é›ªçƒæˆ˜å’Œè¶³çƒæ¯”èµ›ä¸­ä¸å…¶ä»–æ™ºèƒ½ä½“ç«äº‰**ã€‚

å¬èµ·æ¥å¾ˆæœ‰è¶£å§ï¼Ÿä¸‹æ¬¡è§ï¼

æœ€åæˆ‘ä»¬å¾ˆä¹æ„å»å¬å¬ä½ å¯¹äºè¯¥è¯¾ç¨‹çš„æƒ³æ³•æ¥å¸®åŠ©æˆ‘ä»¬æå‡ä»–ã€‚å¦‚æœä½ æœ‰ä»»ä½•åé¦ˆè¯· ğŸ‘‰ [å¡«å†™è¡¨æ ¼](https://forms.gle/BzKXWzLAGZESGNaE9)ã€‚

æœŸå¾…åœ¨ç¬¬äº”å•å…ƒå†è§ï¼ğŸ”¥

### ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ· ğŸ¤—
