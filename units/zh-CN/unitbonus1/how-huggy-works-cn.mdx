# Huggy 的工作原理 [[how-huggy-works]]

Huggy 是由 Hugging Face 制作的深度强化学习环境，基于 [Unity MLAgents 团队的项目 Puppo the Corgi](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit）。
此环境是使用 [Unity 游戏引擎](https://unity.com/) 和 [MLAgents](https://github.com/Unity-Technologies/ml-agents) 创建的。 ML-Agents 是 Unity 游戏引擎的工具包，它允许我们**使用 Unity 创建环境或使用预制环境来训练我们的智能体**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.jpg" alt="Huggy" width="100%">

在这种环境下，我们的目标是训练 Huggy **捡起我们扔的棍子。这意味着他需要正确地朝着棍子移动**。

## Huggy 感知到的状态空间。 [[state-space]]
Huggy 不会“看到”他的环境。相反，我们向他提供有关环境的信息：

- 目标（摇杆）位置
- 自己和目标之间的相对位置
- 他腿的方向。

鉴于所有这些信息，Huggy 可以**使用他的策略来确定接下来要采取的行动来实现他的目标**。

## 动作空间，Huggy 可以执行的动作 [[action-space]]
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-action.jpg" alt="Huggy action" width="100%">

**关节电机驱动 Huggy 腿**。这意味着为了达到目标， Huggy 需要**学会正确地旋转每条腿的关节马达，这样他才能移动**。

## 奖励函数 [[reward-function]]

奖励函数的设计是为了让 ** Huggy 实现他的目标**：拿到棍子。

请记住，强化学习的基础之一是*奖励假设*：目标可以描述为**预期累积奖励**的最大化**。

在这里，我们的目标是 Huggy **朝着棍子走去，但不要旋转太多**。因此，我们的奖励函数必须转化为这个目标。

我们的奖励函数：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/reward.jpg" alt="Huggy reward function" width="100%">

- *定向奖励*：我们**奖励他接近目标**。
- *时间惩罚*：在每个动作中给予的固定时间惩罚，以**迫使他尽快到达棍子**。
- *旋转惩罚*：如果  Huggy  旋转太多并且转得太快**，我们会对  Huggy  进行惩罚。
- *达到目标奖励*：我们奖励 Huggy **达到目标**。

如果你想看看这个奖励函数在数学上是什么样子的，请查看 [Puppo the Corgi presentation](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml -代理工具包）。

## 训练 Huggy 

 Huggy 的目标是**学习正确并尽可能快地朝着目标奔跑**。为此，在每一步并根据环境观察，他需要决定如何旋转他腿的每个关节马达以正确移动（不要旋转太多）并朝向目标。

训练循环如下所示：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-loop.jpg" alt="Huggy loop" width="100%">


训练环境是这样的：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/training-env.jpg" alt="Huggy training env" width="100%">


这是一个**棍子随机产生**的地方。当 Huggy 到达它时，棍子会在其他地方生成。
我们为训练构建了**环境的多个副本**。这有助于通过提供更多样化的体验来加快培训速度。



既然你已经了解了环境的大局，就可以训练 Huggy 去拿棍子了。

为此，我们将使用 [MLAgents](https://github.com/Unity-Technologies/ml-agents)。如果你以前从未使用过它，请不要担心。在本单元中，我们将使用 Google Colab 来训练 Huggy ，然后你就可以加载训练好的 Huggy 并直接在浏览器中与他一起玩。

在以后的单元中，我们将更深入地研究 MLAgent 及其工作原理。但是现在，我们只使用提供的实现来保持简单。