- title: (ì‘ì—…ì¤‘) Unit 0. Welcome to the course
  sections:
  - local: ../en/unit0/introduction
    title: (ì‘ì—…ì¤‘) Welcome to the course ğŸ¤—
  - local: ../en/unit0/setup
    title: (ì‘ì—…ì¤‘) Setup
  - local: ../en/unit0/discord101
    title: (ì‘ì—…ì¤‘) Discord 101
- title: (ì‘ì—…ì¤‘) Unit 1. Introduction to Deep Reinforcement Learning
  sections:
  - local: ../en/unit1/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit1/what-is-rl
    title: (ì‘ì—…ì¤‘) What is Reinforcement Learning?
  - local: ../en/unit1/rl-framework
    title: (ì‘ì—…ì¤‘) The Reinforcement Learning Framework
  - local: ../en/unit1/tasks
    title: (ì‘ì—…ì¤‘) The type of tasks
  - local: ../en/unit1/exp-exp-tradeoff
    title: (ì‘ì—…ì¤‘) The Exploration/ Exploitation tradeoff
  - local: ../en/unit1/two-methods
    title: (ì‘ì—…ì¤‘) The two main approaches for solving RL problems
  - local: ../en/unit1/deep-rl
    title: (ì‘ì—…ì¤‘) The â€œDeepâ€ in Deep Reinforcement Learning
  - local: ../en/unit1/summary
    title: (ì‘ì—…ì¤‘) Summary
  - local: ../en/unit1/glossary
    title: (ì‘ì—…ì¤‘) Glossary
  - local: ../en/unit1/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
  - local: ../en/unit1/quiz
    title: (ì‘ì—…ì¤‘) Quiz
  - local: ../en/unit1/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit1/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Bonus Unit 1. Introduction to Deep Reinforcement Learning with Huggy
  sections:
  - local: ../en/unitbonus1/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unitbonus1/how-huggy-works
    title: (ì‘ì—…ì¤‘) How Huggy works?
  - local: ../en/unitbonus1/train
    title: (ì‘ì—…ì¤‘) Train Huggy
  - local: ../en/unitbonus1/play
    title: (ì‘ì—…ì¤‘) Play with Huggy
  - local: ../en/unitbonus1/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
- title: (ì‘ì—…ì¤‘) Live 1. How the course work, Q&A, and playing with Huggy
  sections:
  - local: ../en/live1/live1
    title: (ì‘ì—…ì¤‘) Live 1. How the course work, Q&A, and playing with Huggy ğŸ¶
- title: (ì‘ì—…ì¤‘) Unit 2. Introduction to Q-Learning
  sections:
  - local: ../en/unit2/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit2/what-is-rl
    title: (ì‘ì—…ì¤‘) What is RL? A short recap
  - local: ../en/unit2/two-types-value-based-methods
    title: (ì‘ì—…ì¤‘) The two types of value-based methods
  - local: ../en/unit2/bellman-equation
    title: (ì‘ì—…ì¤‘) The Bellman Equation, simplify our value estimation
  - local: ../en/unit2/mc-vs-td
    title: (ì‘ì—…ì¤‘) Monte Carlo vs Temporal Difference Learning
  - local: ../en/unit2/mid-way-recap
    title: (ì‘ì—…ì¤‘) Mid-way Recap
  - local: ../en/unit2/mid-way-quiz
    title: (ì‘ì—…ì¤‘) Mid-way Quiz
  - local: ../en/unit2/q-learning
    title: (ì‘ì—…ì¤‘) Introducing Q-Learning
  - local: ../en/unit2/q-learning-example
    title: (ì‘ì—…ì¤‘) A Q-Learning example
  - local: ../en/unit2/q-learning-recap
    title: (ì‘ì—…ì¤‘) Q-Learning Recap
  - local: ../en/unit2/glossary
    title: (ì‘ì—…ì¤‘) Glossary
  - local: ../en/unit2/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
  - local: ../en/unit2/quiz2
    title: (ì‘ì—…ì¤‘) Q-Learning Quiz
  - local: ../en/unit2/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit2/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Unit 3. Deep Q-Learning with Atari Games
  sections:
  - local: ../en/unit3/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit3/from-q-to-dqn
    title: (ì‘ì—…ì¤‘) From Q-Learning to Deep Q-Learning
  - local: ../en/unit3/deep-q-network
    title: (ì‘ì—…ì¤‘) The Deep Q-Network (DQN)
  - local: ../en/unit3/deep-q-algorithm
    title: (ì‘ì—…ì¤‘) The Deep Q Algorithm
  - local: ../en/unit3/glossary
    title: (ì‘ì—…ì¤‘) Glossary
  - local: ../en/unit3/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
  - local: ../en/unit3/quiz
    title: (ì‘ì—…ì¤‘) Quiz
  - local: ../en/unit3/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit3/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Bonus Unit 2. Automatic Hyperparameter Tuning with Optuna
  sections:
  - local: ../en/unitbonus2/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unitbonus2/optuna
    title: (ì‘ì—…ì¤‘) Optuna
  - local: ../en/unitbonus2/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
- title: (ì‘ì—…ì¤‘) Unit 4. Policy Gradient with PyTorch
  sections:
  - local: ../en/unit4/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit4/what-are-policy-based-methods
    title: (ì‘ì—…ì¤‘) What are the policy-based methods?
  - local: ../en/unit4/advantages-disadvantages
    title: (ì‘ì—…ì¤‘) The advantages and disadvantages of policy-gradient methods
  - local: ../en/unit4/policy-gradient
    title: (ì‘ì—…ì¤‘) Diving deeper into policy-gradient
  - local: ../en/unit4/pg-theorem
    title: (ì‘ì—…ì¤‘) (Optional) the Policy Gradient Theorem
  - local: ../en/unit4/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
  - local: ../en/unit4/quiz
    title: (ì‘ì—…ì¤‘) Quiz
  - local: ../en/unit4/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit4/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Unit 5. Introduction to Unity ML-Agents
  sections:
  - local: ../en/unit5/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit5/how-mlagents-works
    title: (ì‘ì—…ì¤‘) How ML-Agents works?
  - local: ../en/unit5/snowball-target
    title: (ì‘ì—…ì¤‘) The SnowballTarget environment
  - local: ../en/unit5/pyramids
    title: (ì‘ì—…ì¤‘) The Pyramids environment
  - local: ../en/unit5/curiosity
    title: (ì‘ì—…ì¤‘) (Optional) What is curiosity in Deep Reinforcement Learning?
  - local: ../en/unit5/hands-on
    title: (ì‘ì—…ì¤‘) Hands-on
  - local: ../en/unit5/bonus
    title: (ì‘ì—…ì¤‘) Bonus. Learn to create your own environments with Unity and MLAgents
  - local: ../en/unit5/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
- title: (ì‘ì—…ì¤‘) Unit 6. Actor Critic methods with Robotics environments
  sections:
  - local: ../en/unit6/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit6/variance-problem
    title: (ì‘ì—…ì¤‘) The Problem of Variance in Reinforce
  - local: ../en/unit6/advantage-actor-critic
    title: (ì‘ì—…ì¤‘) Advantage Actor Critic (A2C)
  - local: ../en/unit6/hands-on
    title: (ì‘ì—…ì¤‘) Advantage Actor Critic (A2C) using Robotics Simulations with PyBullet and Panda-Gym ğŸ¤–
  - local: ../en/unit6/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit6/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Unit 7. Introduction to Multi-Agents and AI vs AI
  sections:
  - local: ../en/unit7/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit7/introduction-to-marl
    title: (ì‘ì—…ì¤‘) An introduction to Multi-Agents Reinforcement Learning (MARL)
  - local: ../en/unit7/multi-agent-setting
    title: (ì‘ì—…ì¤‘) Designing Multi-Agents systems
  - local: ../en/unit7/self-play
    title: (ì‘ì—…ì¤‘) Self-Play
  - local: ../en/unit7/hands-on
    title: (ì‘ì—…ì¤‘) Let's train our soccer team to beat your classmates' teams (AI vs. AI)
  - local: ../en/unit7/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit7/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Unit 8. Part 1 Proximal Policy Optimization (PPO)
  sections:
  - local: ../en/unit8/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit8/intuition-behind-ppo
    title: (ì‘ì—…ì¤‘) The intuition behind PPO
  - local: ../en/unit8/clipped-surrogate-objective
    title: (ì‘ì—…ì¤‘) Introducing the Clipped Surrogate Objective Function
  - local: ../en/unit8/visualize
    title: (ì‘ì—…ì¤‘) Visualize the Clipped Surrogate Objective Function
  - local: ../en/unit8/hands-on-cleanrl
    title: (ì‘ì—…ì¤‘) PPO with CleanRL
  - local: ../en/unit8/conclusion
    title: (ì‘ì—…ì¤‘) Conclusion
  - local: ../en/unit8/additional-readings
    title: (ì‘ì—…ì¤‘) Additional Readings
- title: (ì‘ì—…ì¤‘) Unit 8. Part 2 Proximal Policy Optimization (PPO) with Doom
  sections:
  - local: ../en/unit8/introduction-sf
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unit8/hands-on-sf
    title: (ì‘ì—…ì¤‘) PPO with Sample Factory and Doom
  - local: ../en/unit8/conclusion-sf
    title: (ì‘ì—…ì¤‘) Conclusion
- title: (ì‘ì—…ì¤‘) Bonus Unit 3. Advanced Topics in Reinforcement Learning
  sections:
  - local: ../en/unitbonus3/introduction
    title: (ì‘ì—…ì¤‘) Introduction
  - local: ../en/unitbonus3/model-based
    title: (ì‘ì—…ì¤‘) Model-Based Reinforcement Learning
  - local: ../en/unitbonus3/offline-online
    title: (ì‘ì—…ì¤‘) Offline vs. Online Reinforcement Learning
  - local: ../en/unitbonus3/rlhf
    title: (ì‘ì—…ì¤‘) Reinforcement Learning from Human Feedback
  - local: ../en/unitbonus3/decision-transformers
    title: (ì‘ì—…ì¤‘) Decision Transformers and Offline RL
  - local: ../en/unitbonus3/language-models
    title: (ì‘ì—…ì¤‘) Language models in RL
  - local: ../en/unitbonus3/curriculum-learning
    title: (ì‘ì—…ì¤‘) (Automatic) Curriculum Learning for RL
  - local: ../en/unitbonus3/envs-to-try
    title: (ì‘ì—…ì¤‘) Interesting environments to try
  - local: ../en/unitbonus3/godotrl
    title: (ì‘ì—…ì¤‘) An Introduction to Godot RL
  - local: ../en/unitbonus3/rl-documentation
    title: (ì‘ì—…ì¤‘) Brief introduction to RL documentation
- title: (ì‘ì—…ì¤‘) Certification and congratulations
  sections:
  - local: ../en/communication/conclusion
    title: (ì‘ì—…ì¤‘) Congratulations
  - local: ../en/communication/certification
    title: (ì‘ì—…ì¤‘) Get your certificate of completion
