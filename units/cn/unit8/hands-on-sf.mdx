# 实践练习：高级深度强化学习。使用Sample Factory从像素级别玩Doom游戏

<CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
notebooks={[
  {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb"}
  ]}
  askForHelpUrl="http://hf.co/join/discord" />

Colab笔记本：
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit8/unit8_part2.ipynb)

# 单元8 第2部分：高级深度强化学习。使用Sample Factory从像素级别玩Doom游戏

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail2.png" alt="Thumbnail"/>

在本笔记本中，我们将学习如何训练一个深度神经网络，在基于Doom游戏的3D环境中收集物品，下面展示了最终策略的视频。我们使用[Sample Factory](https://www.samplefactory.dev/)（PPO算法的异步实现）来训练这个策略。

请注意以下几点：

*   [Sample Factory](https://www.samplefactory.dev/)是一个高级强化学习框架，**仅在Linux和Mac上运行**（不支持Windows）。

*  该框架在**具有多个CPU核心的GPU机器**上表现最佳，可以达到每秒10万次交互的速度。标准Colab笔记本上可用的资源**限制了这个库的性能**。因此，在这种设置下的速度**并不反映真实世界的性能**。
* Sample Factory的基准测试在多种设置中都有提供，如果你想了解更多，请查看[示例](https://github.com/alex-petrenko/sample-factory/tree/master/sf_examples)。


```python
from IPython.display import HTML

HTML(
    """<video width="640" height="480" controls>
  <source src="https://huggingface.co/edbeeching/doom_health_gathering_supreme_3333/resolve/main/replay.mp4"
  type="video/mp4">Your browser does not support the video tag.</video>"""
)
```

为了在[认证过程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)中验证这个实践练习，你需要推送一个模型：

- `doom_health_gathering_supreme` 获得 >= 5 的结果。

要查找你的结果，请前往[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)并找到你的模型，**结果 = 平均奖励 - 奖励的标准差**

如果你找不到你的模型，**请转到页面底部并点击刷新按钮**

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

## 设置GPU 💪

- 为了**加速智能体的训练，我们将使用GPU**。为此，请转到`Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1" />

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2" />

在开始训练我们的智能体之前，让我们**研究一下我们将要使用的库和环境**。

## Sample Factory

[Sample Factory](https://www.samplefactory.dev/)是**最快的强化学习库之一，专注于策略梯度（PPO）的高效同步和异步实现**。

Sample Factory经过**充分测试，被许多研究人员和实践者使用**，并且得到积极维护。我们的实现**在各种领域都能达到最先进的性能，同时最小化强化学习实验的训练时间和硬件需求**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactoryenvs.png" alt="Sample factory"/>

### 主要特点

- 高度优化的算法[架构](https://www.samplefactory.dev/06-architecture/overview/)，实现最大学习吞吐量
- [同步和异步](https://www.samplefactory.dev/07-advanced-topics/sync-async/)训练模式
- [串行（单进程）模式](https://www.samplefactory.dev/07-advanced-topics/serial-mode/)便于调试
- 在基于CPU和[GPU加速环境](https://www.samplefactory.dev/09-environment-integrations/isaacgym/)中都能达到最佳性能
- 单智能体和多智能体训练、自我对弈、支持在一个或多个GPU上[同时训练多个策略](https://www.samplefactory.dev/07-advanced-topics/multi-policy-training/)
- 基于种群的训练（[PBT](https://www.samplefactory.dev/07-advanced-topics/pbt/)）
- 离散、连续、混合动作空间
- 基于向量、基于图像、字典观察空间
- 通过解析动作/观察空间规范自动创建模型架构。支持[自定义模型架构](https://www.samplefactory.dev/03-customization/custom-models/)
- 设计为可导入到其他项目中，[自定义环境](https://www.samplefactory.dev/03-customization/custom-environments/)是一等公民
- 详细的[WandB和Tensorboard摘要](https://www.samplefactory.dev/05-monitoring/metrics-reference/)，[自定义指标](https://www.samplefactory.dev/05-monitoring/custom-metrics/)
- [HuggingFace 🤗 集成](https://www.samplefactory.dev/10-huggingface/huggingface/)（将训练好的模型和指标上传到Hub）
- [多个](https://www.samplefactory.dev/09-environment-integrations/mujoco/)[示例](https://www.samplefactory.dev/09-environment-integrations/atari/)[环境](https://www.samplefactory.dev/09-environment-integrations/vizdoom/)[集成](https://www.samplefactory.dev/09-environment-integrations/dmlab/)，带有调优参数和训练好的模型

以上所有策略都可在🤗 hub上获取。搜索标签[sample-factory](https://huggingface.co/models?library=sample-factory&sort=downloads)

### Sample-factory如何工作

Sample-factory是**社区可用的最高度优化的强化学习实现之一**。

它通过**生成多个进程来运行rollout工作器、推理工作器和学习工作器**。

这些*工作器***通过共享内存进行通信，这降低了进程之间的通信成本**。

*rollout工作器*与环境交互并将观察结果发送给*推理工作器*。

*推理工作器*查询策略的固定版本并**将动作发送回rollout工作器**。

在*k*步之后，rollout工作器将经验轨迹发送给学习工作器，**学习工作器使用这些经验来更新智能体的策略网络**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactory.png" alt="Sample factory"/>

### Sample-factory中的Actor Critic模型

Sample Factory中的Actor Critic模型由三个组件组成：

- **编码器（Encoder）** - 处理输入观察（图像、向量）并将其映射到向量。这是你最可能想要自定义的模型部分。
- **核心（Core）** - 整合来自一个或多个编码器的向量，可以选择在基于记忆的智能体中包含单层或多层LSTM/GRU。
- **解码器（Decoder）** - 在计算策略和价值输出之前，对模型核心的输出应用额外的层。

该库被设计为自动支持任何观察和动作空间。用户可以轻松添加自定义模型。你可以在[文档](https://www.samplefactory.dev/03-customization/custom-models/#actor-critic-models-in-sample-factory)中了解更多信息。

## ViZDoom

[ViZDoom](https://vizdoom.cs.put.edu.pl/)是**Doom引擎的一个开源Python接口**。

该库由波兹南理工大学计算机科学研究所的Marek Wydmuch和Michal Kempka于2016年创建。

该库**支持在多种场景中直接从屏幕像素训练智能体**，包括团队死亡竞赛，如下面的视频所示。由于ViZDoom环境基于90年代创建的游戏，它可以在现代硬件上以加速速度运行，**使我们能够相当快速地学习复杂的AI行为**。

该库包括以下功能：

- 多平台（Linux、macOS、Windows）
- Python和C++的API
- [OpenAI Gym](https://www.gymlibrary.dev/)环境包装器
- 易于创建自定义场景（提供可视化编辑器、脚本语言和示例）
- 异步和同步的单人和多人模式
- 轻量级（几MB）且快速（在同步模式下，单线程可达7000 fps）
- 可自定义的分辨率和渲染参数
- 访问深度缓冲区（3D视觉）
- 自动标记帧中可见的游戏对象
- 访问音频缓冲区
- 访问角色/对象列表和地图几何
- 离屏渲染和情节记录
- 异步模式下的时间缩放

## 我们首先需要安装ViZDoom环境所需的一些依赖项

现在我们的Colab运行时已经设置好了，我们可以开始安装在Linux上运行ViZDoom所需的依赖项。

如果你在Mac上的机器上跟进，你需要按照[github页面](https://github.com/Farama-Foundation/ViZDoom/blob/master/doc/Quickstart.md#-quickstart-for-macos-and-anaconda3-python-36)上的安装说明进行操作。

```python
# Install ViZDoom deps from
# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux

apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \
nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \
libopenal-dev timidity libwildmidi-dev unzip ffmpeg

# Boost libraries
apt-get install libboost-all-dev

# Lua binding dependencies
apt-get install liblua5.1-dev
```

## Then we can install Sample Factory and ViZDoom

- This can take 7min

```bash
pip install sample-factory
pip install vizdoom
```

## Setting up the Doom Environment in sample-factory

```python
import functools

from sample_factory.algo.utils.context import global_model_factory
from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args
from sample_factory.envs.env_utils import register_env
from sample_factory.train import run_rl

from sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder
from sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults
from sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec


# Registers all the ViZDoom environments
def register_vizdoom_envs():
    for env_spec in DOOM_ENVS:
        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)
        register_env(env_spec.name, make_env_func)


# Sample Factory allows the registration of a custom Neural Network architecture
# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details
def register_vizdoom_models():
    global_model_factory().register_encoder_factory(make_vizdoom_encoder)


def register_vizdoom_components():
    register_vizdoom_envs()
    register_vizdoom_models()


# parse the command line args and create a config
def parse_vizdoom_cfg(argv=None, evaluation=False):
    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)
    # parameters specific to Doom envs
    add_doom_env_args(parser)
    # override Doom default values for algo parameters
    doom_override_defaults(parser)
    # second parsing pass yields the final configuration
    final_cfg = parse_full_cfg(parser, argv)
    return final_cfg
```

现在设置已经完成，我们可以训练智能体了。我们在这里选择学习一个名为`Health Gathering Supreme`的ViZDoom任务。

### 场景：Health Gathering Supreme（健康收集至尊版）

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/Health-Gathering-Supreme.png" alt="Health-Gathering-Supreme"/>



这个场景的目标是**教会智能体如何在不知道什么能让它生存的情况下生存**。智能体只知道**生命是宝贵的**，死亡是不好的，所以**它必须学习什么能延长它的存在，以及它的健康与生存是相连的**。

地图是一个包含墙壁的矩形，地面是绿色的酸性地板，**会定期伤害玩家**。最初，地图上均匀分布着一些医疗包。每隔一段时间，一个新的医疗包会从天空掉落。**医疗包可以恢复玩家健康的一部分**——为了生存，智能体需要拾取它们。在玩家死亡或超时后，回合结束。

进一步配置：
- 生存奖励 = 1
- 3个可用按钮：向左转，向右转，向前移动
- 1个可用游戏变量：HEALTH（健康值）
- 死亡惩罚 = 100

你可以在[这里](https://github.com/Farama-Foundation/ViZDoom/tree/master/scenarios)了解更多关于ViZDoom中可用场景的信息。

还有一些为ViZDoom创建的更复杂的场景，比如[这个github页面](https://github.com/edbeeching/3d_control_deep_rl)上详述的场景。



## 训练智能体

- 我们将训练智能体4000000步。这将花费大约20分钟

```python
## Start the training, this should take around 15 minutes
register_vizdoom_components()

# The scenario we train on today is health gathering
# other scenarios include "doom_basic", "doom_two_colors_easy", "doom_dm", "doom_dwango5", "doom_my_way_home", "doom_deadly_corridor", "doom_defend_the_center", "doom_defend_the_line"
env = "doom_health_gathering_supreme"
cfg = parse_vizdoom_cfg(
    argv=[f"--env={env}", "--num_workers=8", "--num_envs_per_worker=4", "--train_for_env_steps=4000000"]
)

status = run_rl(cfg)
```

## 让我们看看训练好的策略的表现并输出智能体的视频

```python
from sample_factory.enjoy import enjoy

cfg = parse_vizdoom_cfg(
    argv=[f"--env={env}", "--num_workers=1", "--save_video", "--no_render", "--max_num_episodes=10"], evaluation=True
)
status = enjoy(cfg)
```

## 现在让我们可视化智能体的表现

```python
from base64 import b64encode
from IPython.display import HTML

mp4 = open("/content/train_dir/default_experiment/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```

智能体已经学到了一些东西，但它的表现可以更好。我们显然需要训练更长时间。但让我们先将这个模型上传到Hub。

## 现在让我们将你的检查点和视频上传到Hugging Face Hub




要能够与社区分享你的模型，还有三个步骤需要遵循：

1️⃣ （如果还没有完成）创建一个HF账户 ➡ https://huggingface.co/join

2️⃣ 登录并从Hugging Face网站获取你的认证令牌。
- 创建一个新令牌（https://huggingface.co/settings/tokens）**具有写入权限**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token" />

- 复制令牌
- 运行下面的单元格并粘贴令牌

如果你不想使用Google Colab或Jupyter Notebook，你需要使用这个命令代替：`huggingface-cli login`

```python
from huggingface_hub import notebook_login
notebook_login()
!git config --global credential.helper store
```

```python
from sample_factory.enjoy import enjoy

hf_username = "ThomasSimonini"  # insert your HuggingFace username here

cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=10",
        "--max_num_frames=100000",
        "--push_to_hub",
        f"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme",
    ],
    evaluation=True,
)
status = enjoy(cfg)
```

## 让我们加载另一个模型




这个智能体的表现很好，但我们可以做得更好！让我们从hub下载并可视化一个经过10B时间步训练的智能体。

```bash
#download the agent from the hub
python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir
```

```bash
ls train_dir/doom_health_gathering_supreme_2222
```

```python
env = "doom_health_gathering_supreme"
cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=10",
        "--experiment=doom_health_gathering_supreme_2222",
        "--train_dir=train_dir",
    ],
    evaluation=True,
)
status = enjoy(cfg)
```

```python
mp4 = open("/content/train_dir/doom_health_gathering_supreme_2222/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```

## 一些额外的挑战 🏆：Doom死亡竞赛

训练一个智能体玩Doom死亡竞赛**需要在比Colab更强大的机器上花费很多小时**。

幸运的是，我们**已经在这个场景中训练了一个智能体，它可以在🤗 Hub上获取！**让我们下载模型并可视化智能体的表现。

```python
# 从Hub下载智能体
python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_deathmatch_bots_2222 -d ./train_dir
```

由于智能体需要很长时间，视频生成可能需要**10分钟**。

```python
from sample_factory.enjoy import enjoy

register_vizdoom_components()
env = "doom_deathmatch_bots"
cfg = parse_vizdoom_cfg(
    argv=[
        f"--env={env}",
        "--num_workers=1",
        "--save_video",
        "--no_render",
        "--max_num_episodes=1",
        "--experiment=doom_deathmatch_bots_2222",
        "--train_dir=train_dir",
    ],
    evaluation=True,
)
status = enjoy(cfg)
mp4 = open("/content/train_dir/doom_deathmatch_bots_2222/replay.mp4", "rb").read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(
    """
<video width=640 controls>
      <source src="%s" type="video/mp4">
</video>
"""
    % data_url
)
```


你**可以尝试在这个环境中训练你的智能体**，使用上面的代码，但不是在colab上。
**祝你好运 🤞**

如果你更喜欢简单一点的场景，**为什么不尝试在其他ViZDoom场景中训练，比如`doom_deadly_corridor`或`doom_defend_the_center`。**



---


这就结束了最后一个单元。但我们还没有完成！🤗 接下来的**奖励部分包含了深度强化学习中一些最有趣、最先进和最前沿的工作**。

## 持续学习，保持精彩 🤗
