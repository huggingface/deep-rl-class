# 基于模型的强化学习 (MBRL)

基于模型的强化学习与其无模型对应物的唯一区别在于学习一个*动态模型*，但这对决策方式产生了重大的下游影响。

动态模型通常对环境转换动态进行建模，\\( s_{t+1} = f_\theta (s_t, a_t) \\)，但在这个框架中也可以使用逆动态模型（从状态映射到动作）或奖励模型（预测奖励）等。


## 简单定义

- 有一个智能体反复尝试解决问题，**积累状态和动作数据**。
- 利用这些数据，智能体创建一个结构化学习工具，*动态模型*，用于推理世界。
- 通过动态模型，智能体**通过预测未来来决定如何行动**。
- 通过这些行动，**智能体收集更多数据，改进模型，并有望改进未来的行动**。

## 学术定义

基于模型的强化学习(MBRL)遵循智能体在环境中交互的框架，**学习该环境的模型**，然后**利用该模型进行控制（做决策）**。

具体来说，智能体在由转换函数\\( s_{t+1} = f (s_t , a_t) \\)控制的马尔可夫决策过程(MDP)中行动，并在每一步返回奖励\\( r(s_t, a_t) \\)。通过收集的数据集\\( D := \{s_i, a_i, s_{i+1}, r_i\} \\)，智能体学习一个模型\\( s_{t+1} = f_\theta (s_t , a_t) \\) **以最小化转换的负对数似然**。

我们使用基于学习的动态模型的基于样本的模型预测控制(MPC)，该方法优化从均匀分布\\( U(a) \\)采样的一组动作在有限递归预测范围\\( \tau \\)上的期望奖励（参见[论文](https://arxiv.org/pdf/2002.04523)或[论文](https://arxiv.org/pdf/2012.09156.pdf)或[论文](https://arxiv.org/pdf/2009.01221.pdf)）。

## 进一步阅读

有关MBRL的更多信息，我们建议您查看以下资源：

- 一篇关于[调试MBRL的博客文章](https://www.natolambert.com/writing/debugging-mbrl)。
- 一篇[关于MBRL的最新综述论文](https://arxiv.org/abs/2006.16712)。

## 作者

本节由<a href="https://twitter.com/natolambert"> Nathan Lambert </a>撰写
