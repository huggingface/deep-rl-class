# 离线与在线强化学习

深度强化学习(RL)是一个**构建决策智能体的框架**。这些智能体旨在通过**尝试错误与环境交互并接收奖励作为唯一反馈**来学习最优行为（策略）。

智能体的目标是**最大化其累积奖励**，称为回报。因为RL基于*奖励假设*：所有目标都可以描述为**期望累积奖励的最大化**。

深度强化学习智能体**通过经验批次学习**。问题是，它们如何收集这些经验？：

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/offlinevsonlinerl.gif" alt="Unit bonus 3 thumbnail" />
<figcaption>在线和离线设置中强化学习的比较，图片来自<a href="https://offline-rl.github.io/">这篇文章</a></figcaption>
</figure>

- 在*在线强化学习*中，也就是我们在本课程中学习的内容，智能体**直接收集数据**：它通过**与环境交互**收集一批经验。然后，它立即使用这些经验（或通过某种回放缓冲区）来学习（更新其策略）。

但这意味着你要么**直接在真实世界中训练你的智能体，要么拥有一个模拟器**。如果你没有，你需要构建一个，这可能非常复杂（如何在环境中反映真实世界的复杂现实？），昂贵且不安全（如果模拟器有可能提供竞争优势的缺陷，智能体将利用它们）。

- 另一方面，在*离线强化学习*中，智能体只**使用从其他智能体或人类示范中收集的数据**。它**不与环境交互**。

过程如下：
- **使用一个或多个策略和/或人类交互创建数据集**。
- 在这个数据集上**运行离线RL**来学习策略

这种方法有一个缺点：*反事实查询问题*。如果我们的智能体**决定做一些我们没有数据的事情，我们该怎么办？**例如，在十字路口向右转但我们没有这个轨迹。

关于这个话题存在一些解决方案，但如果你想了解更多关于离线强化学习的信息，你可以[观看这个视频](https://www.youtube.com/watch?v=k08N5a0gG0A)

## 进一步阅读

欲了解更多信息，我们建议您查看以下资源：

- [离线强化学习，Sergei Levine的演讲](https://www.youtube.com/watch?v=qgZPZREor5I)
- [离线强化学习：教程、综述和开放问题的展望](https://arxiv.org/abs/2005.01643)

## 作者

本节由<a href="https://twitter.com/ThomasSimonini"> Thomas Simonini</a>撰写
