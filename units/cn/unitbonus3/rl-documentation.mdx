# 强化学习文档简介

在这个高级主题中，我们解决了这个问题：**我们应该如何监控和跟踪那些在真实世界中训练并与人类交互的强大强化学习智能体？**

随着机器学习系统对现代生活的影响日益增加，**对这些系统进行文档记录的呼声也在增长**。

这样的文档可以涵盖诸如使用的训练数据——存储位置、收集时间、参与人员等方面——或模型优化框架——架构、评估指标、相关论文等——以及更多内容。

如今，模型卡片和数据表正变得越来越普及。例如，在Hub上
（参见[此处](https://huggingface.co/docs/hub/model-cards)的文档）。

如果你点击[Hub上的热门模型](https://huggingface.co/models)，你可以了解其创建过程。

这些特定于模型和数据的日志旨在在创建模型或数据集时完成，当这些模型在未来被构建到不断发展的系统中时，它们不会被更新。
​
## 奖励报告的动机

强化学习系统从根本上设计为基于奖励和时间的测量进行优化。
虽然奖励函数的概念可以很好地映射到许多被充分理解的监督学习领域（通过损失函数），
但对机器学习系统如何随时间演变的理解是有限的。

为此，作者引入了[*强化学习的奖励报告*](https://www.notion.so/Brief-introduction-to-RL-documentation-b8cbda5a6f5242338e0756e6bef72af4)（这个简洁的命名旨在模仿流行的论文*模型卡片用于模型报告*和*数据集的数据表*）。
目标是提出一种专注于**奖励的人为因素**和**随时间变化的反馈系统**的文档类型。

基于Mitchell等人和Gebru等人提出的[模型卡片](https://arxiv.org/abs/1810.03993)和[数据表](https://arxiv.org/abs/1803.09010)的文档框架，我们主张AI系统需要奖励报告。

**奖励报告**是为提议的RL部署划定设计选择的活文档。

然而，关于这个框架对不同RL应用的适用性、系统可解释性的障碍，以及已部署的监督机器学习系统与RL中使用的顺序决策之间的共鸣，仍有许多问题。

至少，奖励报告为RL从业者提供了一个机会，让他们深思这些问题，并开始决定如何在实践中解决它们。
​
## 用文档捕捉时间行为

专为RL和反馈驱动的ML系统设计的文档的核心部分是*变更日志*。变更日志更新来自设计者的信息（更改的训练参数、数据等）以及来自用户的注意到的变化（有害行为、意外响应等）。

变更日志伴随着鼓励监控这些效果的更新触发器。

## 贡献

一些最具影响力的RL驱动系统本质上是多利益相关者的，并且在私人公司的闭门后面。
这些公司在很大程度上没有监管，所以文档记录的负担落在了公众身上。

如果你有兴趣贡献，我们正在为流行的机器学习系统构建奖励报告，并在[GitHub](https://github.com/RewardReports/reward-reports)上公开记录。
​
欲了解更多信息，你可以访问奖励报告[论文](https://arxiv.org/abs/2204.10817)
或查看[一个示例报告](https://github.com/RewardReports/reward-reports/tree/main/examples)。

## 作者

本节由<a href="https://twitter.com/natolambert"> Nathan Lambert </a>撰写
