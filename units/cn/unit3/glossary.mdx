# 术语表 

这是一个社区创建的术语表。欢迎贡献！

- **表格方法：** 一种问题类型，其中状态和动作空间小到足以使用数组和表格来表示近似值函数。 
**Q学习**是表格方法的一个例子，因为使用表格来表示不同状态-动作对的值。

- **深度Q学习：** 一种训练神经网络的方法，用于近似给定状态下，该状态的不同可能动作的**Q值**。
当观察空间太大而无法应用表格Q学习方法时使用。 

- **时间限制** 是当环境状态由帧表示时出现的一个困难。单独一帧本身不提供时间信息。 
为了获取时间信息，我们需要将多个帧**堆叠**在一起。  

- **深度Q学习的阶段：**
  - **采样：** 执行动作，并将观察到的经验元组存储在**回放记忆**中。
  - **训练：** 随机选择元组批次，神经网络使用梯度下降更新其权重。 
  
- **稳定深度Q学习的解决方案：**
  - **经验回放：** 创建一个回放记忆来保存可以在训练期间重复使用的经验样本。 
  这允许智能体多次从相同的经验中学习。此外，它帮助智能体在获取新经验时避免忘记以前的经验。
  - **从回放缓冲区随机采样** 允许消除观察序列中的相关性，防止动作值剧烈振荡或灾难性发散。

  - **固定Q目标：** 为了计算**Q目标**，我们需要使用贝尔曼方程估计下一个状态的折扣最优**Q值**。问题
  是相同的网络权重被用来计算**Q目标**和**Q值**。这意味着每次我们修改**Q值**时，**Q目标**也随之移动。
  为了避免这个问题，使用一个具有固定参数的单独网络来估计时间差分目标。目标网络通过在特定的**C步**后从
  我们的深度Q网络复制参数来更新。 
  
  - **双DQN：** 处理**Q值过高估计**的方法。这个解决方案使用两个网络来解耦动作选择和目标**值生成**：
     - **DQN网络** 用于选择下一个状态的最佳动作（具有最高**Q值**的动作）
     - **目标网络** 用于计算在下一个状态采取该动作的目标**Q值**。 
这种方法减少了**Q值**的过高估计，它有助于更快地训练并获得更稳定的学习。

如果你想改进课程，你可以[开启一个拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表的实现要感谢：

- [Dario Paez](https://github.com/dario248)
