# 中途回顾 [[mid-way-recap]]

在深入研究Q-Learning之前，让我们总结一下我们刚刚学到的内容。

我们有两种类型的基于价值的函数：

- 状态价值函数：输出如果**智能体从给定状态开始，并在此后永远按照策略行动**所能获得的预期回报。
- 动作价值函数：输出如果**智能体从给定状态开始，在该状态采取给定动作**，然后在此后永远按照策略行动所能获得的预期回报。
- 在基于价值的方法中，我们不是学习策略，而是**手动定义策略**，然后学习价值函数。如果我们有一个最优价值函数，我们**将拥有一个最优策略。**

更新价值函数有两种方法：

- 使用*蒙特卡洛方法*，我们从完整的回合更新价值函数，因此**使用这个回合的实际折扣回报。**
- 使用*TD学习方法*，我们从一个步骤更新价值函数，用**称为TD目标的估计回报**替换未知的\\(G_t\\)。


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg" alt="Summary"/>
