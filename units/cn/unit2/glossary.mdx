# 术语表 [[glossary]]

这是一个社区创建的术语表。欢迎贡献！


### 寻找最优策略的策略

- **基于策略的方法。** 策略通常使用神经网络进行训练，以根据状态选择要采取的动作。在这种情况下，是神经网络输出智能体应该采取的动作，而不是使用价值函数。根据从环境接收到的经验，神经网络将被重新调整并提供更好的动作。
- **基于价值的方法。** 在这种情况下，训练一个价值函数来输出状态或状态-动作对的价值，这将代表我们的策略。然而，这个价值并不定义智能体应该采取什么动作。相反，我们需要根据价值函数的输出指定智能体的行为。例如，我们可以决定采用一种策略，始终采取导致最大奖励的动作（贪婪策略）。总之，策略是一个贪婪策略（或用户做出的任何决定），它使用价值函数的值来决定要采取的动作。

### 在基于价值的方法中，我们可以找到两种主要策略

- **状态价值函数。** 对于每个状态，状态价值函数是如果智能体从该状态开始并遵循策略直到结束所获得的预期回报。
- **动作价值函数。** 与状态价值函数相比，动作价值函数计算的是对于每个状态和动作对，如果智能体从该状态开始，采取该动作，然后永远遵循策略所获得的预期回报。

### Epsilon-贪婪策略：

- 强化学习中常用的策略，涉及平衡探索和利用。
- 以1-epsilon的概率选择具有最高预期奖励的动作。
- 以epsilon的概率选择随机动作。
- Epsilon通常随时间减少，以将重点转向利用。

### 贪婪策略：

- 总是选择基于当前对环境的了解，预期会导致最高奖励的动作。（仅利用）
- 始终选择具有最高预期奖励的动作。
- 不包含任何探索。
- 在具有不确定性或未知最优动作的环境中可能不利。

### 离策略与在策略算法

- **离策略算法：** 在训练时和推理时使用不同的策略
- **在策略算法：** 在训练和推理期间使用相同的策略

### 蒙特卡洛和时序差分学习策略

- **蒙特卡洛(MC)：** 在情节结束时学习。使用蒙特卡洛，我们等到情节结束，然后根据完整的情节更新价值函数（或策略函数）。

- **时序差分(TD)：** 在每一步学习。使用时序差分学习，我们在每一步更新价值函数（或策略函数），而不需要完整的情节。

如果你想改进课程，你可以[提交Pull Request。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表的实现得益于：

- [Ramón Rueda](https://github.com/ramon-rd)
- [Hasarindu Perera](https://github.com/hasarinduperera/)
- [Arkady Arkhangorodsky](https://github.com/arkadyark/)
