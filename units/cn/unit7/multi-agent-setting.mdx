# 设计多智能体系统

在本节中，你将观看由<a href="https://www.youtube.com/channel/UCq0imsn84ShAe9PBOFnoIrg">Brian Douglas</a>制作的关于多智能体的精彩介绍。

<Youtube id="qgb0gyrpiGk" />


在这个视频中，Brian讨论了如何设计多智能体系统。他特别以吸尘器的多智能体系统为例，并提出问题：**它们如何相互合作**？

我们有两种解决方案来设计这种多智能体强化学习系统（MARL）。

## 去中心化系统

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/decentralized.png" alt="去中心化"/>
<figcaption>
来源：<a href="https://www.youtube.com/watch?v=qgb0gyrpiGk">多智能体强化学习介绍</a>
</figcaption>
</figure>

在去中心化学习中，**每个智能体都是独立于其他智能体进行训练的**。在给出的例子中，每个吸尘器学习尽可能多地清洁地方，**而不关心其他吸尘器（智能体）在做什么**。

好处是**由于智能体之间不共享信息，这些吸尘器可以像我们训练单个智能体一样被设计和训练**。

这里的想法是**我们的训练智能体将其他智能体视为环境动态的一部分**。而不是作为智能体。

然而，这种技术的一个很大的缺点是它会**使环境变得非平稳**，因为随着其他智能体也在环境中交互，底层的马尔可夫决策过程会随时间变化。
这对许多强化学习算法来说是有问题的，**因为它们无法在非平稳环境中达到全局最优**。

## 中心化方法

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/centralized.png" alt="中心化"/>
<figcaption>
来源：<a href="https://www.youtube.com/watch?v=qgb0gyrpiGk">多智能体强化学习介绍</a>
</figcaption>
</figure>

在这种架构中，**我们有一个高级过程来收集智能体的经验**：经验缓冲区。我们将使用这些经验**来学习一个共同的策略**。

例如，在吸尘器的例子中，观察将是：
- 吸尘器的覆盖地图。
- 所有吸尘器的位置。

我们使用这种集体经验**来训练一个策略，该策略将以对整体最有利的方式移动所有三个机器人**。所以每个机器人都在从它们的共同经验中学习。
我们现在有一个平稳的环境，因为所有的智能体都被视为一个更大的实体，并且它们知道其他智能体策略的变化（因为它与它们自己的相同）。

如果我们总结一下：

- 在*去中心化方法*中，我们**独立对待所有智能体，而不考虑其他智能体的存在**。
  - 在这种情况下，所有智能体**将其他智能体视为环境的一部分**。
  - **这是一个非平稳环境条件**，因此无法保证收敛。

- 在*中心化方法*中：
  - **从所有智能体中学习单一策略**。
  - 以环境的当前状态作为输入，策略输出联合行动。
  - 奖励是全局性的。
