# 自我对弈：训练对抗性游戏中竞争性智能体的经典技术

现在我们已经学习了多智能体的基础知识，我们准备深入研究。正如在介绍中提到的，我们将**在SoccerTwos这个2对2游戏的对抗性游戏中训练智能体**。

<figure>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif" alt="SoccerTwos"/>

<figcaption>这个环境由<a href="https://github.com/Unity-Technologies/ml-agents">Unity MLAgents团队</a>制作</figcaption>

</figure>

## 什么是自我对弈？

在对抗性游戏中正确训练智能体可能**相当复杂**。

一方面，我们需要找到如何获得一个训练良好的对手来与你的训练智能体对抗。另一方面，如果你找到一个非常好的训练对手，当对手太强时，你的智能体如何改进其策略？

想想一个刚开始学习足球的孩子。与一个非常优秀的足球运动员对抗将毫无用处，因为这将太难获胜，或者至少偶尔获得球权。所以这个孩子将不断失败，没有时间学习一个好的策略。

最好的解决方案是**拥有一个与智能体水平相当的对手，并随着智能体提升自己的水平而提升其水平**。因为如果对手太强，我们将学不到任何东西；如果对手太弱，我们将过度学习对抗更强对手时无用的行为。

这种解决方案被称为*自我对弈*。在自我对弈中，**智能体使用自己的前一个副本（其策略的副本）作为对手**。这样，智能体将与同一水平的智能体对抗（具有挑战性但不会太多），有机会逐渐改进其策略，然后随着它变得更好而更新其对手。这是一种引导对手并逐步增加对手复杂性的方法。

这与人类在竞争中学习的方式相同：

- 我们开始与同等水平的对手训练
- 然后我们从中学习，当我们获得一些技能后，我们可以与更强的对手进一步发展。

我们在自我对弈中也做同样的事情：

- 我们**以我们的智能体的副本作为对手开始**，这样，这个对手处于相似的水平。
- 我们**从中学习**，当我们获得一些技能后，我们**用我们训练策略的更新版本更新我们的对手**。

自我对弈背后的理论并不是什么新鲜事物。它已经在五十年代由Arthur Samuel的跳棋玩家系统和1995年由Gerald Tesauro的TD-Gammon使用。如果你想了解更多关于自我对弈的历史，[请查看Andrew Cohen的这篇非常好的博客文章](https://blog.unity.com/technology/training-intelligent-adversaries-using-self-play-with-ml-agents)

## MLAgents中的自我对弈

自我对弈已集成到MLAgents库中，并由我们将要研究的多个超参数管理。但正如文档中所解释的，主要关注点是**最终策略的技能水平和通用性与学习稳定性之间的权衡**。

对抗一组缓慢变化或不变的对手，多样性低的训练**会导致更稳定的训练。但如果变化太慢，就有过拟合的风险。**

所以我们需要控制：

- **我们多久更换一次对手**，通过`swap_steps`和`team_change`参数。
- **保存的对手数量**，通过`window`参数。`window`的较大值意味着智能体的对手池将包含更多样化的行为，因为它将包含来自训练运行早期的策略。
- **与当前自我对弈与从池中抽样的对手对抗的概率**，通过`play_against_latest_model_ratio`参数。`play_against_latest_model_ratio`的较大值表示智能体将更频繁地与当前对手对抗。
- **在保存新对手之前的训练步骤数**，通过`save_steps`参数。`save_steps`的较大值将产生一组覆盖更广泛技能水平和可能的游戏风格的对手，因为策略接收更多的训练。

要获取有关这些超参数的更多详细信息，你绝对需要[查看文档的这部分](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Training-Configuration-File.md#self-play)


## ELO评分来评估我们的智能体

### 什么是ELO评分？

在对抗性游戏中，跟踪**累积奖励并不总是一个有意义的指标来跟踪学习进度：**因为这个指标**仅取决于对手的技能**。

相反，我们使用***ELO评分系统***（以Arpad Elo命名），它计算零和游戏中给定人群中两个玩家之间的**相对技能水平**。

在零和游戏中：一个智能体获胜，另一个智能体失败。这是一种数学表示，其中每个参与者的效用增益或损失**与其他参与者的效用增益或损失完全平衡**。我们之所以称之为零和游戏，是因为效用总和等于零。

这个ELO（从特定分数开始：通常是1200）可能最初会下降，但应该在训练过程中逐渐增加。

Elo系统是**从对其他玩家的失败和平局中推断出来的**。这意味着玩家的评分取决于**他们对手的评分以及对他们的得分结果**。

Elo定义了一个Elo分数，它是玩家在零和游戏中的相对技能。**我们说相对是因为它取决于对手的表现**。

核心思想是将玩家的表现视为**一个正态分布的随机变量**。

两个玩家之间的评分差异作为**比赛结果的预测因子**。如果玩家获胜，但获胜的概率很高，它只会从对手那里赢得很少的分数，因为这意味着它比对手强得多。

每场比赛后：

- 获胜的玩家从失败的玩家那里**获取分数**。
- 分数的数量**由两个玩家评分的差异决定（因此是相对的）**。
    - 如果评分较高的玩家获胜 → 从评分较低的玩家那里获取很少的分数。
    - 如果评分较低的玩家获胜 → 从评分较高的玩家那里获取大量分数。
    - 如果是平局 → 评分较低的玩家从评分较高的玩家那里获得一些分数。

所以如果A和B的评分分别为Ra和Rb，那么**预期分数**由以下公式给出：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo1.png" alt="ELO评分"/>

然后，在游戏结束时，我们需要更新玩家的实际Elo分数。我们使用**与玩家表现超出或不足的程度成比例的线性调整**。

我们还定义了每场比赛的最大调整评分：K因子。

- 对于大师级别，K=16。
- 对于较弱的玩家，K=32。

如果玩家A有Ea分但得到了Sa分，那么玩家的评分使用以下公式更新：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/elo2.png" alt="ELO评分"/>

### 示例

如果我们举个例子：

玩家A的评分为2600

玩家B的评分为2300

- 我们首先计算预期分数：

\(E_{A} = \frac{1}{1+10^{(2300-2600)/400}} = 0.849 \)

\(E_{B} = \frac{1}{1+10^{(2600-2300)/400}} = 0.151 \)

- 如果组织者确定K=16且A获胜，新的评分将是：

\\(ELO_A = 2600 + 16*(1-0.849) = 2602 \\)

\\(ELO_B = 2300 + 16*(0-0.151) = 2298 \\)

- 如果组织者确定K=16且B获胜，新的评分将是：

\\(ELO_A = 2600 + 16*(0-0.849) = 2586 \\)

\\(ELO_B = 2300 + 16 *(1-0.151) = 2314 \\)


### 优势

使用ELO评分有多个优势：

- 分数**始终平衡**（当出现意外结果时会交换更多分数，但总和始终相同）。
- 这是一个**自我修正的系统**，因为如果一个玩家击败一个弱玩家，他们只会赢得很少的分数。
- 它**适用于团队游戏**：我们计算每个团队的平均值并在Elo中使用它。

### 缺点

- ELO**不考虑团队中每个人的个人贡献**。
- 评分通缩：**保持良好评分需要随着时间的推移保持技能**。
- **无法比较历史上的评分**。
