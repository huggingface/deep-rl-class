# 总结 [[summary]]

这里有很多信息！让我们总结一下：

- 强化学习是一种从行动中学习的计算方法。我们构建一个智能体，它通过**与环境的试错交互**学习，并接收奖励（负面或正面）作为反馈。

- 任何强化学习智能体的目标都是最大化其期望累积奖励（也称为期望回报），因为强化学习基于**奖励假设**，即**所有目标都可以描述为期望累积奖励的最大化。**

- 强化学习过程是一个输出**状态、动作、奖励和下一状态**序列的循环。

- 为了计算期望累积奖励（期望回报），我们对奖励进行折扣：更早到来的奖励（在游戏开始时）**更有可能发生，因为它们比长期未来奖励更可预测。**

- 要解决强化学习问题，你需要**找到一个最优策略**。策略是你的智能体的"大脑"，它告诉我们**在给定状态下采取什么动作。**最优策略是**给你带来最大化期望回报的动作的策略。**

- 有两种方法可以找到你的最优策略：
    1. 通过直接训练你的策略：**基于策略的方法。**
    2. 通过训练一个价值函数，告诉我们智能体在每个状态下将获得的期望回报，并使用这个函数来定义我们的策略：**基于价值的方法。**

- 最后，我们谈论深度强化学习是因为我们引入了**深度神经网络来估计要采取的动作（基于策略）或估计状态的价值（基于价值）**，因此得名"深度"。
