# 术语表 [[glossary]]

这是一个社区创建的术语表。欢迎贡献！

### 智能体

智能体通过**试错学习做决策，从环境中获得奖励和惩罚**。

### 环境

环境是一个模拟世界，**智能体可以通过与之交互来学习**。

### 马尔可夫性质

它意味着我们的智能体采取的行动**仅取决于当前状态，而与过去的状态和行动无关**。

### 观察/状态

- **状态**：对世界状态的完整描述。
- **观察**：对环境/世界状态的部分描述。

### 动作

- **离散动作**：有限数量的动作，如左、右、上和下。
- **连续动作**：无限可能的动作；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能发生的动作。

### 奖励和折扣

- **奖励**：强化学习中的基本因素。告诉智能体所采取的行动是好还是坏。
- 强化学习算法专注于最大化**累积奖励**。
- **奖励假设**：强化学习问题可以被表述为（累积）回报的最大化。
- 进行**折扣**是因为在开始时获得的奖励比长期奖励更可能发生，因为它们比长期奖励更可预测。

### 任务

- **情节性**：有起点和终点。
- **连续性**：有起点但没有终点。

### 探索与利用权衡

- **探索**：通过尝试随机动作来探索环境，从环境中获取反馈/回报/奖励。
- **利用**：利用我们对环境的了解来获得最大奖励。
- **探索-利用权衡**：它平衡了我们想要**探索**环境的程度和我们想要**利用**我们对环境所知道的信息的程度。

### 策略

- **策略**：被称为智能体的大脑。它告诉我们在给定状态下应该采取什么行动。
- **最优策略**：当智能体按照它行动时，**最大化**预期回报的策略。它是通过*训练*学习的。

### 基于策略的方法：

- 解决强化学习问题的一种方法。
- 在这种方法中，直接学习策略。
- 将每个状态映射到该状态下的最佳相应动作。或者映射到该状态下可能动作集合的概率分布。

### 基于价值的方法：

- 解决强化学习问题的另一种方法。
- 在这里，我们不是训练策略，而是训练一个**价值函数**，将每个状态映射到处于该状态的预期价值。

欢迎贡献 🤗

如果你想改进课程，你可以[提交一个Pull Request。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表的实现要感谢：

- [@lucifermorningstar1305](https://github.com/lucifermorningstar1305)
- [@daspartho](https://github.com/daspartho)
- [@misza222](https://github.com/misza222)

