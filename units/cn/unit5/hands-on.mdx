# 实践练习

<CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
notebooks={[
  {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb"}
  ]}
  askForHelpUrl="http://hf.co/join/discord" />


我们已经学习了ML-Agents是什么以及它如何工作。我们还研究了将要使用的两个环境。现在我们准备训练我们的智能体了！

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/envs.png" alt="Environments" />

为了在认证过程中验证这个实践练习，你**只需要将训练好的模型推送到Hub**。
为了验证这个实践练习，**没有需要达到的最低结果**。但如果你想获得不错的结果，可以尝试达到以下目标：

- 对于[金字塔环境](https://huggingface.co/spaces/unity/ML-Agents-Pyramids)：平均奖励 = 1.75
- 对于[雪球目标环境](https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget)：平均奖励 = 15 或在一个回合中击中30个目标。

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

**要开始实践练习，请点击"在Colab中打开"按钮** 👇：

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit5/unit5.ipynb)

我们强烈**建议学生使用Google Colab进行实践练习**，而不是在个人电脑上运行它们。

通过使用Google Colab，**你可以专注于学习和实验，而不必担心**设置环境的技术方面。

# 第5单元：ML-Agents入门

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png" alt="Thumbnail"/>

在这个笔记本中，你将了解ML-Agents并训练两个智能体。

- 第一个智能体将学习**向出现的目标发射雪球**。
- 第二个智能体需要按下按钮生成一个金字塔，然后导航到金字塔，将其推倒，**并移动到顶部的金砖**。为了做到这一点，它需要探索环境，我们将使用一种称为好奇心的技术。

之后，你将能够**直接在浏览器中观看你的智能体玩游戏**。

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

⬇️ 这里是**你在本单元结束时将实现的内容**的示例。 ⬇️

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.gif" alt="Pyramids"/>

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif" alt="SnowballTarget"/>

### 🎮 环境：

- [金字塔](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)
- 雪球目标

### 📚 RL库：

- [ML-Agents](https://github.com/Unity-Technologies/ml-agents)

我们一直在努力改进我们的教程，所以**如果你在这个笔记本中发现一些问题**，请[在GitHub仓库上提出issue](https://github.com/huggingface/deep-rl-class/issues)。

## 本笔记本的目标 🏆

在完成本笔记本后，你将：

- 理解**ML-Agents**如何工作以及环境库。
- 能够**在Unity环境中训练智能体**。

## 先决条件 🏗️
在深入学习本笔记本之前，你需要：

🔲 📚 **通过[阅读第5单元了解ML-Agents是什么以及它如何工作](https://huggingface.co/deep-rl-course/unit5/introduction)**  🤗

# 让我们训练我们的智能体 🚀

## 设置GPU 💪

- 为了**加速智能体的训练，我们将使用GPU**。为此，请转到`Runtime > Change Runtime type`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1" />

- `Hardware Accelerator > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2" />

## 克隆仓库 🔽

- 我们需要克隆包含**ML-Agents**的仓库。

```bash
# 克隆仓库（可能需要3分钟）
git clone --depth 1 https://github.com/Unity-Technologies/ml-agents
```

## 设置虚拟环境 🔽

- 为了让**ML-Agents**在Colab中成功运行，Colab的Python版本必须满足库的Python要求。

- 我们可以在`setup.py`文件的`python_requires`参数下检查支持的Python版本。这些文件是设置**ML-Agents**库所必需的，可以在以下位置找到：
  - `/content/ml-agents/ml-agents/setup.py`
  - `/content/ml-agents/ml-agents-envs/setup.py`

- Colab当前的Python版本（可以使用`!python --version`检查）与库的`python_requires`参数不匹配，因此安装可能会悄无声息地失败，并在稍后执行相同命令时导致如下错误：
  - `/bin/bash: line 1: mlagents-learn: command not found`
  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`

- 为了解决这个问题，我们将创建一个与**ML-Agents**库兼容的Python版本的虚拟环境。

`注意：` *为了未来的兼容性，如果Colab的Python版本不兼容，请始终检查安装文件中的`python_requires`参数，并在下面的脚本中将虚拟环境设置为支持的最高Python版本*

```bash
# Colab当前的Python版本（与ML-Agents不兼容）
!python --version
```

```bash
# 安装virtualenv并创建虚拟环境
!pip install virtualenv
!virtualenv myenv

# 下载并安装Miniconda
!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
!chmod +x Miniconda3-latest-Linux-x86_64.sh
!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local

# 激活Miniconda并安装Python 3.10.12版本
!source /usr/local/bin/activate
!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # 在这里指定版本

# 设置Python和conda路径的环境变量
!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/
!export CONDA_PREFIX=/usr/local/envs/myenv
```

```bash
# 新虚拟环境中的Python版本（与ML-Agents兼容）
!python --version
```

## 安装依赖项 🔽

```bash
# 进入仓库并安装包（可能需要3分钟）
%cd ml-agents
pip3 install -e ./ml-agents-envs
pip3 install -e ./ml-agents
```

## 雪球目标 ⛄

如果你需要复习这个环境如何工作，请查看此部分 👉
https://huggingface.co/deep-rl-course/unit5/snowball-target

### 下载并将环境zip文件移动到`./training-envs-executables/linux/`

- 我们的环境可执行文件在一个zip文件中。
- 我们需要下载它并将其放置到`./training-envs-executables/linux/`
- 我们使用linux可执行文件，因为我们使用colab，而colab机器的操作系统是Ubuntu（linux）

```bash
# 在这里，我们创建training-envs-executables和linux目录
mkdir ./training-envs-executables
mkdir ./training-envs-executables/linux
```

我们使用`wget`从https://github.com/huggingface/Snowball-Target下载SnowballTarget.zip文件

```bash
wget "https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip" -O ./training-envs-executables/linux/SnowballTarget.zip
```

我们解压可执行文件.zip文件

```bash
unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip
```

确保你的文件可访问

```bash
chmod -R 755 ./training-envs-executables/linux/SnowballTarget
```

### 定义雪球目标配置文件
- 在ML-Agents中，你在config.yaml文件中**定义训练超参数**。

有多个超参数。要更好地理解它们，你应该阅读[文档](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)中对每个超参数的解释


你需要在./content/ml-agents/config/ppo/中创建一个`SnowballTarget.yaml`配置文件

我们将给你一个初步版本的配置（复制并粘贴到你的`SnowballTarget.yaml文件`中），**但你应该修改它**。

```yaml
behaviors:
  SnowballTarget:
    trainer_type: ppo
    summary_freq: 10000
    keep_checkpoints: 10
    checkpoint_interval: 50000
    max_steps: 200000
    time_horizon: 64
    threaded: true
    hyperparameters:
      learning_rate: 0.0003
      learning_rate_schedule: linear
      batch_size: 128
      buffer_size: 2048
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
```

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config1.png" alt="Config SnowballTarget"/>
<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config2.png" alt="Config SnowballTarget"/>

作为实验，尝试修改一些其他超参数。Unity提供了非常[好的文档，在这里解释每个超参数](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md)。

现在你已经创建了配置文件并理解了大多数超参数的作用，我们准备训练我们的智能体了 🔥。

### 训练智能体

要训练我们的智能体，我们需要**启动mlagents-learn并选择包含环境的可执行文件**。

我们定义四个参数：

1. `mlagents-learn <config>`：超参数配置文件的路径。
2. `--env`：环境可执行文件的位置。
3. `--run_id`：你想给训练运行ID起的名称。
4. `--no-graphics`：在训练期间不启动可视化。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png" alt="MlAgents learn"/>

训练模型并使用`--resume`标志在中断的情况下继续训练。

> 如果你使用`--resume`，第一次可能会失败。尝试重新运行代码块以绕过错误。

训练将根据你的配置需要10到35分钟。去喝杯☕️，你值得拥有 🤗。

```bash
mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id="SnowballTarget1" --no-graphics
```

### 将智能体推送到Hugging Face Hub

- 现在我们已经训练了我们的智能体，我们**准备将它推送到Hub并在浏览器上观看它的表现🔥。**

为了能够与社区分享你的模型，还有三个步骤需要遵循：

1️⃣ （如果你还没有完成）创建一个HF账户 ➡ https://huggingface.co/join

2️⃣ 登录并从Hugging Face网站存储你的认证令牌。
- 创建一个新令牌（https://huggingface.co/settings/tokens）**具有写入权限**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token" />

- 复制令牌
- 运行下面的单元格并粘贴令牌

```python
from huggingface_hub import notebook_login

notebook_login()
```

如果你不想使用Google Colab或Jupyter Notebook，你需要使用这个命令：`huggingface-cli login`

然后我们需要运行`mlagents-push-to-hf`。

我们定义四个参数：

1. `--run-id`：训练运行ID的名称。
2. `--local-dir`：智能体保存的位置，它是results/`run_id名称`，所以在我的例子中是results/First Training。
3. `--repo-id`：你想创建或更新的Hugging Face仓库的名称。它总是`你的huggingface用户名/仓库名称`
如果仓库不存在，**它将自动创建**。
4. `--commit-message`：由于HF仓库是git仓库，你需要提供一个提交消息。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentspushtohub.png" alt="Push to Hub"/>

例如：

`mlagents-push-to-hf  --run-id="SnowballTarget1" --local-dir="./results/SnowballTarget1" --repo-id="ThomasSimonini/ppo-SnowballTarget"  --commit-message="First Push"`

```python
mlagents-push-to-hf  --run-id= # 添加你的运行ID  --local-dir= # 你的本地目录  --repo-id= # 你的仓库ID  --commit-message= # 你的提交消息
```

如果一切正常，你应该在过程结束时看到这个（但URL不同 😆）：



```
Your model is pushed to the hub. You can view your model here: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget
```

这是你模型的链接。它包含一个模型卡片，解释如何使用它，你的Tensorboard和你的配置文件。**最棒的是它是一个git仓库，这意味着你可以有不同的提交，通过新的推送更新你的仓库等。**

但现在是最好的部分：**能够在线观看你的智能体 👀。**

### 观看你的智能体玩游戏 👀

这一步很简单：

1. 前往：https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget

2. 启动游戏并通过点击右下角按钮将其全屏显示

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png" alt="Snowballtarget load"/>

1. 在步骤1中，输入你的用户名（你的用户名区分大小写：例如，我的用户名是ThomasSimonini而不是thomassimonini或ThOmasImoNInI）并点击搜索按钮。

2. 在步骤2中，选择你的模型仓库。

3. 在步骤3中，**选择你想要回放的模型**：
  - 我有多个模型，因为我们每500000个时间步保存一个模型。
  - 但由于我想要最新的一个，我选择`SnowballTarget.onnx`

👉 **尝试不同的模型步骤来查看智能体的改进情况**是很好的。


不要犹豫，在discord的#rl-i-made-this频道分享你的智能体获得的最佳分数 🔥

现在让我们尝试一个更具挑战性的环境，称为金字塔。

## 金字塔 🏆

### 下载并将环境zip文件移动到`./training-envs-executables/linux/`
- 我们的环境可执行文件在一个zip文件中。
- 我们需要下载它并将其放置到`./training-envs-executables/linux/`
- 我们使用linux可执行文件，因为我们使用colab，而colab机器的操作系统是Ubuntu（linux）

我们使用`wget`从https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip下载Pyramids.zip文件

```python
wget "https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip" -O ./training-envs-executables/linux/Pyramids.zip
```

解压它

```python
unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip
```

确保你的文件可访问

```bash
chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids
```

### 修改PyramidsRND配置文件
  
- 与第一个环境不同，第一个是自定义的，**金字塔是由Unity团队制作的**。
- 所以PyramidsRND配置文件已经存在，位于./content/ml-agents/config/ppo/PyramidsRND.yaml
- 你可能会问为什么PyramidsRND中有"RND"。RND代表*随机网络蒸馏（random network distillation）*，这是一种生成好奇心奖励的方法。如果你想了解更多关于这种技术的信息，我们写了一篇文章解释这种技术：https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938

对于这次训练，我们将修改一件事：
- 总训练步数超参数太高，因为我们只需要1M训练步数就能达到基准（平均奖励 = 1.75）。
👉 为此，我们转到config/ppo/PyramidsRND.yaml，**将max_steps更改为1000000。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png" alt="Pyramids config"/>

作为实验，你也应该尝试修改一些其他超参数。Unity提供了非常[好的文档，在这里解释每个超参数](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md)。

我们现在准备训练我们的智能体 🔥。

### 训练智能体

根据你的机器，训练将需要30到45分钟，去喝杯☕️，你值得拥有 🤗。

```python
mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id="Pyramids Training" --no-graphics
```

### 将智能体推送到Hugging Face Hub

- 现在我们已经训练了我们的智能体，我们**准备将它推送到Hub，以便能够在浏览器上观看它的表现🔥。**

```python
mlagents-push-to-hf  --run-id= # 添加你的运行ID  --local-dir= # 你的本地目录  --repo-id= # 你的仓库ID  --commit-message= # 你的提交消息
```

### 观看你的智能体玩游戏 👀

👉 https://huggingface.co/spaces/unity/ML-Agents-Pyramids
  
### 🎁 奖励：为什么不在另一个环境中训练？
  
现在你知道如何使用MLAgents训练智能体，**为什么不尝试另一个环境呢？**

MLAgents提供了17个不同的环境，我们正在构建一些自定义环境。学习的最好方式是自己尝试，玩得开心。

![cover](https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png)

你可以在这里找到目前在Hugging Face上可用环境的完整列表 👉 https://github.com/huggingface/ml-agents#the-environments

用于可视化你的智能体的演示 👉 https://huggingface.co/unity

目前我们已经集成了：
- [蠕虫](https://huggingface.co/spaces/unity/ML-Agents-Worm)演示，你教一条**蠕虫爬行**。
- [步行者](https://huggingface.co/spaces/unity/ML-Agents-Walker)演示，你教一个智能体**朝着目标行走**。

今天的内容就到这里。恭喜你完成本教程！

学习的最好方式是实践和尝试。为什么不尝试另一个环境呢？ML-Agents有18个不同的环境，但你也可以创建自己的环境。查看文档并玩得开心！

我们在第6单元见 ，

## 持续学习，保持精彩 🤗
