# 入门指南：

首先，从[这里](https://huggingface.co/ivan267/imitation-learning-tutorial-godot-project/tree/main)下载项目（点击`GDRL-IL-Project.zip`旁边的下载图标）。压缩文件包含"Starter"和"Complete"两个项目。

入门项目中已经实现了游戏代码并配置了节点。我们将专注于：

- 实现AIController节点的代码，
- 记录专家示范，
- 训练智能体并导出.onnx文件，以便在Godot中进行推理。

### 在Godot中打开入门项目

解压缩文件，打开Godot，点击"Import"并导航到解压缩档案的`Starter\Godot`文件夹。

### 打开机器人场景

<Tip>
你可以在文件系统搜索中搜索"robot"。
</Tip>

这个场景包含几个不同的节点，包括`robot`节点（包含机器人的视觉形状），`CameraXRotation`节点（用于在人类控制模式下使用鼠标"上下"旋转摄像机）。AI智能体不控制这个节点，因为学习任务不需要它。`RaycastSensors`节点包含两个光线投射传感器，帮助智能体"感知"游戏世界的部分内容，包括墙壁、地板等。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit13/open-robot-scene.jpg" alt="打开机器人场景"/>

### 点击AIController3D旁边的滚动图标以打开脚本进行编辑

<Tip>
你可能需要折叠"robot"分支以更容易找到它，或者你可以在`Robot`节点上方的过滤框中输入`aicontroller`。
</Tip>

### 用以下实现替换`get_obs()`和`get_reward()`方法：

```python
func get_obs() -> Dictionary:
	var observations: Array[float] = []
	for raycast_sensor in raycast_sensors:
		observations.append_array(raycast_sensor.get_observation())

	var level_size = 16.0

	var chest_local = to_local(chest.global_position)
	var chest_direction = chest_local.normalized()
	var chest_distance = clampf(chest_local.length(), 0.0, level_size)
	
	var lever_local = to_local(lever.global_position)
	var lever_direction = lever_local.normalized()
	var lever_distance = clampf(lever_local.length(), 0.0, level_size)
		
	var key_local = to_local(key.global_position)
	var key_direction = key_local.normalized()
	var key_distance = clampf(key_local.length(), 0.0, level_size)
	
	var raft_local = to_local(raft.global_position)
	var raft_direction = raft_local.normalized()
	var raft_distance = clampf(raft_local.length(), 0.0, level_size)
	
	var player_speed = player.global_basis.inverse() * player.velocity.limit_length(5.0) / 5.0

	(
		observations
		.append_array(
			[
				chest_direction.x,
				chest_direction.y,
				chest_direction.z,
				chest_distance,
				lever_direction.x,
				lever_direction.y,
				lever_direction.z,
				lever_distance,
				key_direction.x,
				key_direction.y,
				key_direction.z,
				key_distance,
				raft_direction.x,
				raft_direction.y,
				raft_direction.z,
				raft_distance,
				raft.movement_direction_multiplier,
				float(player._is_lever_pulled),
				float(player._is_chest_opened),
				float(player._is_key_collected),
				float(player.is_on_floor()),
				player_speed.x,
				player_speed.y,
				player_speed.z,
			]
		)
	)
	return {"obs": observations}

func get_reward() -> float:
	return reward
```

在`get_obs()`中，我们首先获取在检查器中添加到`AIController3D`节点的两个光线投射传感器的观测值，并将它们添加到观测中，然后我们获取到箱子、杠杆、钥匙和木筏的相对位置向量，将它们分为方向和距离，然后也将它们添加到观测中。

我们还将其他游戏状态信息添加到观测中：

- 杠杆是否已被拉动，
- 钥匙是否已被收集，
- 箱子是否已被打开，
- 玩家是否在地面上（也决定玩家是否可以跳跃），
- 玩家的归一化局部速度。

我们将布尔值（如`_is_lever_pulled`）转换为浮点数（0或1）。

在`get_reward()`中，我们只需要返回当前奖励。

### 用以下实现替换`_physics_process()`和`reset()`方法：

```python
func _physics_process(delta: float) -> void:
	# Reset on timeout, this is implemented in parent class to set needs_reset to true,
	# we are re-implementing here to call player.game_over() that handles the game reset.
	n_steps += 1
	if n_steps > reset_after:
		player.game_over()

	# In training or onnx inference modes, this method will be called by sync node with actions provided,
	# For expert demo recording mode, it will be called without any actions (as we set the actions based on human input),
	# For human control mode the method will not be called, so we call it here without any actions provided.
	if control_mode == ControlModes.HUMAN:
		set_action()

	# Reset the game faster if the lever is not pulled.
	steps_without_lever_pulled += 1
	if steps_without_lever_pulled > 200 and (not player._is_lever_pulled):
		player.game_over()

func reset():
	super.reset()
	steps_without_lever_pulled = 0
```

### **用以下实现替换`get_action_space()`、`get_action()`和`set_action()`方法：**

```python
# Defines the actions for the AI agent ("size": 2 means 2 floats for this action)
func get_action_space() -> Dictionary:
	return {
		"movement": {"size": 2, "action_type": "continuous"},
		"rotation": {"size": 1, "action_type": "continuous"},
		"jump": {"size": 1, "action_type": "continuous"},
		"use_action": {"size": 1, "action_type": "continuous"}
	}

# We return the action values in the same order as defined in get_action_space() (important), but all in one array
# For actions of size 1, we return 1 float in the array, for size 2, 2 floats in the array, etc.
# set_action is called just before get_action by the sync node, so we can read the newly set values
func get_action():
	return [
		# "movement" action values
		player.requested_movement.x,
		player.requested_movement.y,
		# "rotation" action value
		player.requested_rotation.x,
		# "jump" action value (-1 if not requested, 1 if requested)
		-1.0 + 2.0 * float(player.jump_requested),
		# "use_action" action value (-1 if not requested, 1 if requested)
		-1.0 + 2.0 * float(player.use_action_requested)
	]

# Here we set human control and AI control actions to the robot
func set_action(action = null) -> void:
	# If there's no action provided, it means that AI is not controlling the robot (human control),
	if not action:
		# Only rotate if the mouse has moved since the last set_action call
		if previous_mouse_movement == mouse_movement:
			mouse_movement = Vector2.ZERO

		player.requested_movement = Input.get_vector(
			"move_left", "move_right", "move_forward", "move_back"
		)
		player.requested_rotation = mouse_movement

		var use_action = Input.is_action_pressed("requested_action")
		var jump = Input.is_action_pressed("requested_jump")

		player.use_action_requested = use_action
		player.jump_requested = jump

		previous_mouse_movement = mouse_movement	
	else:
		# If there is action provided, we set the actions received from the AI agent 
		player.requested_movement = Vector2(action.movement[0], action.movement[1])
		# The agent only rotates the robot along the Y axis, no need to rotate the camera along X axis
		player.requested_rotation = Vector2(action.rotation[0], 0.0)
		player.jump_requested = bool(action.jump[0] > 0)
		player.use_action_requested = bool(action.use_action[0] > 0)
```

对于`get_action()`（仅在使用演示记录模式时需要），我们需要提供当智能体遇到相同状态时要发送的动作。重要的是值要在正确的范围内（`-1.0到1.0`），这就是为什么我们对布尔状态使用`-1 + 2 * 变量`，并且按照`get_action_space()`中定义的正确顺序。

在演示记录模式下，`set_action()`被调用时不提供动作，因为我们需要根据人类输入设置动作值。在训练/推理模式下，该方法被调用时带有一个`action`参数，其中包含RL模型提供的所有动作的值，所以我们有一个`if/else`来处理这两种情况。

更多信息包含在代码注释中。

### 用以下实现替换`_input`方法：

```python
# Record mouse movement for human and demo_record modes
# We don't directly rotate in input to allow for frame skipping (action_repeat setting) which
# will also be applied to the AI agent in training/inference modes.
func _input(event):
	if not (heuristic == "human" or heuristic == "demo_record"):
		return

	if event is InputEventMouseMotion:
		var movement_scale: float = 0.005
		mouse_movement.y = clampf(event.relative.y * movement_scale, -1.0, 1.0)
		mouse_movement.x = clampf(event.relative.x * movement_scale, -1.0, 1.0)
```

这部分代码在人类控制和演示记录模式下记录鼠标移动。

**最后，保存脚本。我们准备进入下一步。**

### 打开演示记录场景，并点击AIController3D节点

<Tip>
你可以在文件系统搜索中搜索"demo"，并在场景的过滤框中搜索"aicontroller"。
</Tip>

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit13/demo_record_scene.jpg" alt="打开机器人场景"/>


你不需要做任何更改，因为一切都已预设好，但让我们回顾一下你在自己的环境中需要设置的内容：

场景包含修改过的`Level > Robot > AIController3D`节点设置：

- `Control Mode`设置为`Record Expert Demos`
- `Expert Demo Save Path`已填写
- `Action Repeat`设置为与`training_scene`和`onnx_inference_scene`中的`Sync`节点相同的值。这意味着智能体设置的每个动作都会在3个物理帧中重复。`AIController`中的设置为人类输入添加了相同的动作重复（这会引入一些延迟）以匹配相同的行为。这是一个相当低的值，不会引入太多延迟。如果你更改此值，请确保在所有3个地方都进行更改。
- `Remove Last Episode`键允许我们在录制过程中使用一个键来删除失败的回合，而无需重新启动整个会话。例如，如果机器人掉入水中并且游戏重置，我们可以使用此键在录制下一个回合时删除之前录制的回合。它被设置为`R`，但你可以通过点击它，然后点击`Configure`按钮来将其更改为任何键。

在具有挑战性的环境中使录制回合更容易的另一种方法是在录制过程中减慢环境速度。这可以通过点击场景中的`Sync`节点，并调整`Speed Up`属性（默认设置为1）来轻松完成。

### 让我们记录一些演示：

<Tip>
请注意，只有当我们至少录制了一个完整的回合并通过点击"X"或按ALT+F4关闭游戏窗口时，演示才会被保存。使用Godot编辑器中的停止按钮不会保存演示。最好先尝试录制一个回合，然后检查文件系统或Godot项目文件夹中是否看到"expert_demos.json"。
</Tip>

确保你仍然在`demo_record_scene`中，`按F6`，演示录制将开始。

控制：

- 鼠标控制摄像机（如果你需要调整鼠标灵敏度，打开`robot`场景，点击`Robot`节点并调整`Rotation Speed`，在录制演示、训练和推理时保持相同的值），
- `WASD`控制玩家移动，
- `SPACE`跳跃，
- `E`激活杠杆并打开箱子

你可以先进行一些练习以熟悉环境。如果你希望跳过录制演示，你也可以在完成的项目中找到预先录制的演示，并使用那里的`expert_demos.json`文件。

录制的演示应该包括至少22-24个完整成功的回合。在训练阶段也可以使用多个演示文件，所以你不必一次性录制所有演示（你可以使用前面提到的`Expert Demo Save Path`属性更改文件名）。

录制23个回合花了我约10分钟（因为钥匙有2个交替的生成位置，22或24个回合将在演示中提供钥匙位置的均等分布，但这已经相当接近）。当接近杠杆或箱子时，我按住`E`键稍长一些，以确保在靠近这些物体时动作被记录多个步骤。我还通过在下一个回合中按`R`键删除了几个我没有成功完成的回合。

以下是演示录制过程的加速视频：

<video src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit13/demo_record.mp4" type="video/mp4" controls autoplay loop mute />

### 导出游戏进行训练：

你可以使用`Project > Export`从Godot导出游戏。
