# 介绍 [[introduction]]


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png"  alt="缩略图"/>

在第4单元中，我们学习了第一个基于策略的算法，称为**Reinforce**。

在基于策略的方法中，**我们的目标是直接优化策略，而不使用价值函数**。更准确地说，Reinforce属于*基于策略的方法*的一个子类，称为*策略梯度方法*。这个子类通过**使用梯度上升估计最优策略的权重**来直接优化策略。

我们看到Reinforce运行良好。然而，由于我们使用蒙特卡洛采样来估计回报（我们使用整个回合来计算回报），**我们在策略梯度估计中存在显著的方差**。

请记住，策略梯度估计是**回报最陡增加方向**。换句话说，如何更新我们的策略权重，使得导致良好回报的动作有更高的概率被选择。蒙特卡洛方差，我们将在本单元中进一步研究，**导致训练速度变慢，因为我们需要大量样本来减轻它**。

所以今天我们将学习**Actor-Critic方法**，这是一种结合了基于价值和基于策略方法的混合架构，通过以下方式帮助稳定训练并减少方差：
- *一个Actor*，控制**我们的智能体如何行动**（基于策略的方法）
- *一个Critic*，衡量**所采取的动作有多好**（基于价值的方法）


我们将学习这些混合方法之一，优势Actor-Critic（A2C），**并使用Stable-Baselines3在机器人环境中训练我们的智能体**。我们将训练：
- 一个机械臂🦾移动到正确位置。

听起来很激动人心吧？让我们开始吧！
