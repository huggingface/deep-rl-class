# 策略梯度方法的优势和劣势

在这一点上，你可能会问，"但深度Q学习很出色！为什么要使用策略梯度方法？"。为了回答这个问题，让我们研究**策略梯度方法的优势和劣势**。

## 优势

相比于基于价值的方法，策略梯度方法有多种优势。让我们看看其中一些：

### 集成的简单性

我们可以直接估计策略，而无需存储额外的数据（动作值）。

### 策略梯度方法可以学习随机策略

策略梯度方法可以**学习随机策略，而价值函数则不能**。

这有两个后果：

1. 我们**不需要手动实现探索/利用权衡**。由于我们输出的是动作的概率分布，智能体**在探索状态空间时不会总是采取相同的轨迹**。

2. 我们还摆脱了**感知混叠**的问题。感知混叠是指两个状态看起来（或实际上）相同，但需要不同的动作。

让我们举个例子：我们有一个智能吸尘器，其目标是吸尘并避免杀死仓鼠。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg" alt="Hamster 1"/>
</figure>

我们的吸尘器只能感知墙壁的位置。

问题是**两个红色（有颜色的）状态是混叠状态，因为智能体在每个状态都感知到上下墙壁**。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg" alt="Hamster 1"/>
</figure>

在确定性策略下，策略在红色状态下要么总是向右移动，要么总是向左移动。**无论哪种情况都会导致我们的智能体陷入困境，永远无法吸尘**。

在基于价值的强化学习算法下，我们学习一个**准确定性策略**（"贪婪epsilon策略"）。因此，我们的智能体可能**需要花费大量时间才能找到灰尘**。

另一方面，最优随机策略**将在红色（有颜色的）状态下随机向左或向右移动**。因此，**它不会陷入困境，并且将以高概率达到目标状态**。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg" alt="Hamster 1"/>
</figure>

### 策略梯度方法在高维动作空间和连续动作空间中更有效

深度Q学习的问题在于，它们的**预测为每个可能的动作分配一个分数（最大期望未来奖励）**，在每个时间步，给定当前状态。

但如果我们有无限可能的动作怎么办？

例如，对于自动驾驶汽车，在每个状态下，你可以有（几乎）无限的动作选择（将方向盘转动15°、17.2°、19.4°、鸣喇叭等）。**我们需要为每个可能的动作输出一个Q值**！而且**取连续输出的最大动作本身就是一个优化问题**！

相反，使用策略梯度方法，我们输出**动作的概率分布**。

### 策略梯度方法具有更好的收敛特性

在基于价值的方法中，我们使用一个激进的运算符来**改变价值函数：我们取Q估计的最大值**。
因此，如果估计的动作值发生任意小的变化导致不同的动作具有最大值，动作概率可能会发生剧烈变化。

例如，如果在训练过程中，最佳动作是左（Q值为0.22），而在下一个训练步骤中变成了右（因为右的Q值变为0.23），我们就会剧烈地改变策略，因为现在策略大部分时间会选择右而不是左。

另一方面，在策略梯度方法中，随机策略动作偏好（采取动作的概率）**随着时间的推移平滑变化**。

## 劣势

当然，策略梯度方法也有一些劣势：

- **策略梯度方法经常收敛到局部最大值而不是全局最优**。
- 策略梯度进展较慢，**一步一步：可能需要更长时间来训练（效率低）**。
- 策略梯度可能具有高方差。我们将在演员-评论家单元中看到为什么会这样，以及如何解决这个问题。

👉 如果你想深入了解策略梯度方法的优势和劣势，[你可以查看这个视频](https://youtu.be/y3oqOjHilio)。
