# 实践练习

      <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
      notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb"}
        ]}
        askForHelpUrl="http://hf.co/join/discord" />



现在我们已经学习了Reinforce的理论，**你已经准备好使用PyTorch编写你的Reinforce智能体**。你将使用CartPole-v1和PixelCopter测试其鲁棒性。

然后你可以迭代并改进这个实现，以适应更高级的环境。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>
</figure>


为了在认证过程中验证这个实践练习，你需要将训练好的模型推送到Hub并：

- 在`Cartpole-v1`中获得 >= 350 的结果
- 在`PixelCopter`中获得 >= 5 的结果

要查找你的结果，请前往排行榜并找到你的模型，**结果 = 平均奖励 - 奖励的标准差**。**如果你在排行榜上看不到你的模型，请前往排行榜页面底部并点击刷新按钮**。

**如果找不到你的模型，请前往页面底部并点击刷新按钮。**

有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

你可以在这里查看你的进度 👉 https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course


**要开始实践练习，请点击"在Colab中打开"按钮** 👇：

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)

我们强烈**建议学生使用Google Colab进行实践练习**，而不是在个人电脑上运行。

通过使用Google Colab，**你可以专注于学习和实验，而不必担心设置环境的技术方面**。

# 第4单元：使用PyTorch编写你的第一个深度强化学习算法：Reinforce。并测试其鲁棒性 💪

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png" alt="thumbnail"/>


在这个笔记本中，你将从头开始编写你的第一个深度强化学习算法：Reinforce（也称为蒙特卡洛策略梯度）。

Reinforce是一种*基于策略的方法*：一种深度强化学习算法，它**尝试直接优化策略，而不使用动作值函数**。

更准确地说，Reinforce是一种*策略梯度方法*，是*基于策略的方法*的一个子类，旨在**通过使用梯度上升估计最优策略的权重来直接优化策略**。

为了测试其鲁棒性，我们将在2个不同的简单环境中训练它：
- Cartpole-v1
- PixelcopterEnv

⬇️ 这里是**你在本笔记本结束时将实现的示例**。 ⬇️

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>


### 🎮 环境：

- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)
- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)

### 📚 RL库：

- Python
- PyTorch



我们一直在努力改进我们的教程，所以**如果你在这个笔记本中发现一些问题**，请[在GitHub仓库上提出issue](https://github.com/huggingface/deep-rl-class/issues)。

## 本笔记本的目标 🏆

在笔记本结束时，你将：

- 能够**使用PyTorch从头开始编写Reinforce算法**。
- 能够**使用简单环境测试你的智能体的鲁棒性**。
- 能够**将你训练好的智能体推送到Hub**，附带精美的视频回放和评估分数 🔥。

## 先决条件 🏗️

在深入笔记本之前，你需要：

🔲 📚 [通过阅读第4单元学习策略梯度](https://huggingface.co/deep-rl-course/unit4/introduction)

# 让我们从头开始编写Reinforce算法 🔥

## 一些建议 💡

最好在Google Drive上的副本中运行这个colab，这样**如果它超时**，你仍然在Google Drive上保存了笔记本，不需要从头开始填写所有内容。

要做到这一点，你可以按`Ctrl + S`或`文件 > 在Google Drive中保存副本`。

## 设置GPU 💪

- 为了**加速智能体的训练，我们将使用GPU**。要做到这一点，请前往`运行时 > 更改运行时类型`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1" />

- `硬件加速器 > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2" />

## 创建虚拟显示器 🖥

在笔记本中，我们需要生成回放视频。为此，使用colab，**我们需要有一个虚拟屏幕来能够渲染环境**（从而记录帧）。

以下单元将安装库并创建并运行虚拟屏幕 🖥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay
!pip install pyglet==1.5.1
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## 安装依赖项 🔽

第一步是安装依赖项。我们将安装多个：

- `gym`
- `gym-games`：使用PyGame制作的额外gym环境。
- `huggingface_hub`：Hub作为一个中心位置，任何人都可以分享和探索模型和数据集。它具有版本控制、指标、可视化和其他功能，这些功能将使你能够轻松地与他人协作。

你可能想知道为什么我们安装gym而不是gymnasium（gym的更新版本）？**因为我们使用的gym-games尚未更新为支持gymnasium**。

你在这里会遇到的区别：
- 在`gym`中，我们没有`terminated`和`truncated`，只有`done`。
- 在`gym`中，使用`env.step()`返回`state, reward, done, info`

你可以在这里了解更多关于Gym和Gymnasium之间的区别 👉 https://gymnasium.farama.org/content/migration-guide/

你可以在这里查看所有可用的Reinforce模型 👉 https://huggingface.co/models?other=reinforce

你可以在这里找到所有深度强化学习模型 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning


```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt
```

## 导入包 📦

除了导入已安装的库外，我们还导入：

- `imageio`：一个将帮助我们生成回放视频的库



```python
import numpy as np

from collections import deque

import matplotlib.pyplot as plt
%matplotlib inline

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Gym
import gym
import gym_pygame

# Hugging Face Hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
import imageio
```

## 检查我们是否有GPU

- 让我们检查我们是否有GPU
- 如果是这种情况，你应该看到`device:cuda0`

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

```python
print(device)
```

我们现在准备实现我们的Reinforce算法 🔥

# 第一个智能体：玩CartPole-v1 🤖

## 创建CartPole环境并了解它的工作原理

### [环境 🎮](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)

### 为什么我们使用像CartPole-v1这样的简单环境？

正如[强化学习技巧和窍门](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)中所解释的，当你从头实现你的智能体时，你需要**确保它正常工作，并在简单环境中找到bug，然后再深入**，因为在简单环境中找到bug会容易得多。


> 尝试在玩具问题上有一些"生命迹象"


> 通过使其在越来越难的环境中运行来验证实现（你可以将结果与RL zoo进行比较）。对于这一步，你通常需要运行超参数优化。


### CartPole-v1环境

> 一个杆通过一个非驱动关节连接到一个推车上，该推车沿着无摩擦的轨道移动。钟摆直立放置在推车上，目标是通过在推车上施加左右方向的力来平衡杆。


所以，我们从CartPole-v1开始。目标是向左或向右推动推车，**使杆保持平衡**。

如果出现以下情况，回合结束：
- 杆角度大于±12°
- 推车位置大于±2.4
- 回合长度大于500

每个时间步杆保持平衡，我们都会获得+1的奖励 💰。

```python
env_id = "CartPole-v1"
# Create the env
env = gym.make(env_id)

# Create the evaluation env
eval_env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

## 让我们构建Reinforce架构

这个实现基于三个实现：
- [PyTorch官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)
- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)
- [Chris1nexus对集成的改进](https://github.com/huggingface/deep-rl-class/pull/95)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png" alt="Reinforce"/>

所以我们需要：
- 两个全连接层（fc1和fc2）。
- 使用ReLU作为fc1的激活函数
- 使用Softmax输出动作的概率分布

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Create two fully connected layers
       
        ...
    def forward(self, x):
    
    ...
    def act(self, state):
        """
        Given a state, take action
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

### 解决方案

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

我犯了一个错误，你能猜出是在哪里吗？

- 让我们进行一次前向传播来找出问题：

```python
debug_policy = Policy(s_size, a_size, 64).to(device)
debug_policy.act(env.reset())
```

- 这里我们看到错误提示`ValueError: The value argument to log_prob must be a Tensor`

- 这意味着`m.log_prob(action)`中的`action`必须是一个张量**但它不是。**

- 你知道为什么吗？检查act函数并尝试找出为什么它不工作。

建议 💡：这个实现中有问题。记住，对于act函数，**我们希望从动作概率分布中采样一个动作**。


### （真正的）解决方案

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

通过使用CartPole，调试变得更容易，因为**我们知道错误来自我们的集成，而不是来自我们的简单环境**。

- 由于**我们想要从动作概率分布中采样一个动作**，我们不能使用`action = np.argmax(m)`，因为它总是输出具有最高概率的动作。

- 我们需要将其替换为`action = m.sample()`，它将从概率分布P(.|s)中采样一个动作

### 让我们构建Reinforce训练算法
这是Reinforce算法的伪代码：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png" alt="Policy gradient pseudocode"/>



- 当我们计算回报Gt（第6行）时，我们看到我们计算的是**从时间步t开始**的折扣奖励总和。

- 为什么？因为我们的策略应该只**根据后果来强化动作**：所以在采取动作之前获得的奖励是无用的（因为它们不是由于该动作造成的），**只有在动作之后获得的奖励才重要**。

- 在编写代码之前，你应该阅读[不要让过去分散你的注意力](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)这一部分，它解释了为什么我们使用reward-to-go策略梯度。

我们使用[Chris1nexus](https://github.com/Chris1nexus)编写的一种有趣技术来**高效计算每个时间步的回报**。注释解释了这个过程。也不要犹豫[查看PR解释](https://github.com/huggingface/deep-rl-class/pull/95)
但总体思路是**高效计算每个时间步的回报**。

你可能会问的第二个问题是**为什么我们要最小化损失**？我们之前不是讨论过梯度上升而不是梯度下降吗？

- 我们想要最大化我们的效用函数$J(\theta)$，但在PyTorch和TensorFlow中，**最小化目标函数**更好。
    - 所以假设我们想在某个时间步强化动作3。在训练之前，这个动作的概率P是0.25。
    - 所以我们想修改\\(theta\\)使得\\(\pi_\theta(a_3|s; \theta) > 0.25\\)
    - 因为所有P的总和必须为1，最大化\\(pi_\theta(a_3|s; \theta)\\)将**最小化其他动作的概率**。
    - 所以我们应该告诉PyTorch**最小化\\(1 - \pi_\theta(a_3|s; \theta)\\)**。
    - 当\\(\pi_\theta(a_3|s; \theta)\\)接近1时，这个损失函数接近0。
    - 所以我们鼓励梯度最大化\\(\pi_\theta(a_3|s; \theta)\\)


```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes+1):
        saved_log_probs = []
        rewards = []
        state = # TODO: reset the environment
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = # TODO get the action
            saved_log_probs.append(log_prob)
            state, reward, done, _ = # TODO: take an env step
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t

        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...


        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = (returns[0] if len(returns)>0 else 0)
            returns.appendleft(    ) # TODO: complete here

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()

        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))

    return scores
```

#### 解决方案

```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes + 1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as
        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t
        #
        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...

        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = returns[0] if len(returns) > 0 else 0
            returns.appendleft(gamma * disc_return_t + rewards[t])

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()
        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print("Episode {}\tAverage Score: {:.2f}".format(i_episode, np.mean(scores_deque)))

    return scores
```

##  训练它
- 我们现在准备训练我们的智能体。
- 但首先，我们定义一个包含所有训练超参数的变量。
- 你可以（而且应该😉）更改训练参数

```python
cartpole_hyperparameters = {
    "h_size": 16,
    "n_training_episodes": 1000,
    "n_evaluation_episodes": 10,
    "max_t": 1000,
    "gamma": 1.0,
    "lr": 1e-2,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

```python
# Create policy and place it to the device
cartpole_policy = Policy(
    cartpole_hyperparameters["state_space"],
    cartpole_hyperparameters["action_space"],
    cartpole_hyperparameters["h_size"],
).to(device)
cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters["lr"])
```

```python
scores = reinforce(
    cartpole_policy,
    cartpole_optimizer,
    cartpole_hyperparameters["n_training_episodes"],
    cartpole_hyperparameters["max_t"],
    cartpole_hyperparameters["gamma"],
    100,
)
```

## 定义评估方法 📝
- 这里我们定义评估方法，用于测试我们的Reinforce智能体。

```python
def evaluate_agent(env, max_steps, n_eval_episodes, policy):
    """
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param policy: The Reinforce agent
    """
    episode_rewards = []
    for episode in range(n_eval_episodes):
        state = env.reset()
        step = 0
        done = False
        total_rewards_ep = 0

        for step in range(max_steps):
            action, _ = policy.act(state)
            new_state, reward, done, info = env.step(action)
            total_rewards_ep += reward

            if done:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## 评估我们的智能体 📈

```python
evaluate_agent(
    eval_env, cartpole_hyperparameters["max_t"], cartpole_hyperparameters["n_evaluation_episodes"], cartpole_policy
)
```

### 在Hub上发布我们训练好的模型 🔥
现在我们看到训练后取得了不错的结果，我们可以用一行代码将训练好的模型发布到Hub 🤗。

这是一个模型卡片的例子：

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png"/>

### 推送到Hub
#### 请勿修改此代码

```python
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
import imageio

import tempfile

import os
```

```python
def record_video(env, policy, out_directory, fps=30):
    """
    Generate a replay video of the agent
    :param env
    :param Qtable: Qtable of our agent
    :param out_directory
    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
    """
    images = []
    done = False
    state = env.reset()
    img = env.render(mode="rgb_array")
    images.append(img)
    while not done:
        # Take the action (index) that have the maximum expected future reward given that state
        action, _ = policy.act(state)
        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic
        img = env.render(mode="rgb_array")
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```python
def push_to_hub(repo_id,
                model,
                hyperparameters,
                eval_env,
                video_fps=30
                ):
  """
  Evaluate, Generate a video and Upload a model to Hugging Face Hub.
  This method does the complete pipeline:
  - It evaluates the model
  - It generates the model card
  - It generates a replay video of the agent
  - It pushes everything to the Hub

  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
  :param model: the pytorch model we want to save
  :param hyperparameters: training hyperparameters
  :param eval_env: evaluation environment
  :param video_fps: how many frame per seconds to record our video replay
  """

  _, repo_name = repo_id.split("/")
  api = HfApi()

  # Step 1: Create the repo
  repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
  )

  with tempfile.TemporaryDirectory() as tmpdirname:
    local_directory = Path(tmpdirname)

    # Step 2: Save the model
    torch.save(model, local_directory / "model.pt")

    # Step 3: Save the hyperparameters to JSON
    with open(local_directory / "hyperparameters.json", "w") as outfile:
      json.dump(hyperparameters, outfile)

    # Step 4: Evaluate the model and build JSON
    mean_reward, std_reward = evaluate_agent(eval_env,
                                            hyperparameters["max_t"],
                                            hyperparameters["n_evaluation_episodes"],
                                            model)
    # Get datetime
    eval_datetime = datetime.datetime.now()
    eval_form_datetime = eval_datetime.isoformat()

    evaluate_data = {
          "env_id": hyperparameters["env_id"],
          "mean_reward": mean_reward,
          "n_evaluation_episodes": hyperparameters["n_evaluation_episodes"],
          "eval_datetime": eval_form_datetime,
    }

    # Write a JSON file
    with open(local_directory / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = hyperparameters["env_id"]

    metadata = {}
    metadata["tags"] = [
          env_name,
          "reinforce",
          "reinforcement-learning",
          "custom-implementation",
          "deep-rl-class"
      ]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
      )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Reinforce** Agent playing **{env_id}**
  This is a trained model of a **Reinforce** agent playing **{env_id}** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """

    readme_path = local_directory / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
          readme = f.read()
    else:
      readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
      f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path =  local_directory / "replay.mp4"
    record_video(env, model, video_path, video_fps)

    # Step 7. Push everything to the Hub
    api.upload_folder(
          repo_id=repo_id,
          folder_path=local_directory,
          path_in_repo=".",
    )

    print(f"Your model is pushed to the Hub. You can view your model here: {repo_url}")
```

通过使用`push_to_hub`，**你可以评估、记录回放、生成智能体的模型卡片，并将其推送到Hub**。

这样：
- 你可以**展示你的工作** 🔥
- 你可以**可视化你的智能体的游戏过程** 👀
- 你可以**与社区分享一个其他人可以使用的智能体** 💾
- 你可以**访问排行榜 🏆 查看你的智能体与同学相比表现如何** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard


要能够与社区分享你的模型，还需要遵循三个步骤：

1️⃣ （如果尚未完成）创建一个HF账户 ➡ https://huggingface.co/join

2️⃣ 登录，然后你需要从Hugging Face网站存储你的认证令牌。
- 创建一个新令牌（https://huggingface.co/settings/tokens）**具有写入权限**


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token" />



```python
notebook_login()
```

如果你不想使用Google Colab或Jupyter Notebook，你需要使用这个命令：`huggingface-cli login`（或`login`）

3️⃣ 我们现在准备使用`package_to_hub()`函数将训练好的智能体推送到🤗 Hub 🔥

```python
repo_id = ""  # TODO 定义你的仓库ID {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    cartpole_policy,  # 我们想要保存的模型
    cartpole_hyperparameters,  # 超参数
    eval_env,  # 评估环境
    video_fps=30
)
```

现在我们已经测试了我们实现的鲁棒性，让我们尝试一个更复杂的环境：PixelCopter 🚁




## 第二个智能体：PixelCopter 🚁

### 研究PixelCopter环境 👀
- [环境文档](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)


```python
env_id = "Pixelcopter-PLE-v0"
env = gym.make(env_id)
eval_env = gym.make(env_id)
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

观察空间(7) 👀:
- 玩家y位置
- 玩家速度
- 玩家到地面的距离
- 玩家到天花板的距离
- 下一个障碍物与玩家的x距离
- 下一个障碍物顶部y位置
- 下一个障碍物底部y位置

动作空间(2) 🎮:
- 向上（按加速器）
- 什么都不做（不按加速器）

奖励函数 💰:
- 每通过一个垂直障碍物，获得+1的正奖励。每次达到终止状态时，获得-1的负奖励。

### 定义新的策略 🧠
- 由于环境更复杂，我们需要一个更深的神经网络

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Define the three layers here

    def forward(self, x):
        # Define the forward process here
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

#### 解决方案

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, h_size * 2)
        self.fc3 = nn.Linear(h_size * 2, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

### 定义超参数 ⚙️
- 因为这个环境更复杂。
- 特别是隐藏层大小，我们需要更多的神经元。

```python
pixelcopter_hyperparameters = {
    "h_size": 64,
    "n_training_episodes": 50000,
    "n_evaluation_episodes": 10,
    "max_t": 10000,
    "gamma": 0.99,
    "lr": 1e-4,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

###  训练它
- 我们现在准备训练我们的智能体 🔥。

```python
# Create policy and place it to the device
# torch.manual_seed(50)
pixelcopter_policy = Policy(
    pixelcopter_hyperparameters["state_space"],
    pixelcopter_hyperparameters["action_space"],
    pixelcopter_hyperparameters["h_size"],
).to(device)
pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters["lr"])
```

```python
scores = reinforce(
    pixelcopter_policy,
    pixelcopter_optimizer,
    pixelcopter_hyperparameters["n_training_episodes"],
    pixelcopter_hyperparameters["max_t"],
    pixelcopter_hyperparameters["gamma"],
    1000,
)
```

### 在Hub上发布我们训练好的模型 🔥

```python
repo_id = ""  # TODO 定义你的仓库ID {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    pixelcopter_policy,  # 我们想要保存的模型
    pixelcopter_hyperparameters,  # 超参数
    eval_env,  # 评估环境
    video_fps=30
)
```

## 一些额外的挑战 🏆

学习的最佳方式**是自己尝试**！正如你所看到的，当前的智能体表现不是很好。作为第一个建议，你可以训练更多步骤。但也尝试找到更好的参数。

在[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)中，你会找到你的智能体。你能登上榜首吗？

以下是一些攀登排行榜的想法：
* 训练更多步骤
* 通过查看你的同学们所做的工作，尝试不同的超参数 👉 https://huggingface.co/models?other=reinforce
* **将你新训练的模型**推送到Hub 🔥
* **改进实现以适应更复杂的环境**（例如，如何将网络更改为卷积神经网络以处理帧作为观察？）

________________________________________________________________________

**恭喜你完成本单元**！这里有很多信息。
也恭喜你完成教程。你刚刚使用PyTorch从头编写了第一个深度强化学习智能体，并将其分享到Hub上 🥳。

不要犹豫，**通过改进更复杂环境的实现**来迭代本单元（例如，如何将网络更改为卷积神经网络以处理帧作为观察？）。

在下一个单元中，**我们将通过在Unity环境中训练智能体来学习更多关于Unity MLAgents的知识**。这样，你将准备好参加**AI对抗AI挑战，在那里你将训练你的智能体与其他智能体在雪球大战和足球比赛中竞争**。

听起来很有趣吧？下次见！

最后，我们很想**听听你对课程的看法以及我们如何改进它**。如果你有一些反馈，请 👉 [填写这个表单](https://forms.gle/BzKXWzLAGZESGNaE9)

我们在第5单元见！��

### 持续学习，保持精彩 🤗
