# å®è·µç»ƒä¹ 

      <CourseFloatingBanner classNames="absolute z-10 right-0 top-0"
      notebooks={[
        {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit4/unit4.ipynb"}
        ]}
        askForHelpUrl="http://hf.co/join/discord" />



ç°åœ¨æˆ‘ä»¬å·²ç»å­¦ä¹ äº†Reinforceçš„ç†è®ºï¼Œ**ä½ å·²ç»å‡†å¤‡å¥½ä½¿ç”¨PyTorchç¼–å†™ä½ çš„Reinforceæ™ºèƒ½ä½“**ã€‚ä½ å°†ä½¿ç”¨CartPole-v1å’ŒPixelCopteræµ‹è¯•å…¶é²æ£’æ€§ã€‚

ç„¶åä½ å¯ä»¥è¿­ä»£å¹¶æ”¹è¿›è¿™ä¸ªå®ç°ï¼Œä»¥é€‚åº”æ›´é«˜çº§çš„ç¯å¢ƒã€‚

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>
</figure>


ä¸ºäº†åœ¨è®¤è¯è¿‡ç¨‹ä¸­éªŒè¯è¿™ä¸ªå®è·µç»ƒä¹ ï¼Œä½ éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubå¹¶ï¼š

- åœ¨`Cartpole-v1`ä¸­è·å¾— >= 350 çš„ç»“æœ
- åœ¨`PixelCopter`ä¸­è·å¾— >= 5 çš„ç»“æœ

è¦æŸ¥æ‰¾ä½ çš„ç»“æœï¼Œè¯·å‰å¾€æ’è¡Œæ¦œå¹¶æ‰¾åˆ°ä½ çš„æ¨¡å‹ï¼Œ**ç»“æœ = å¹³å‡å¥–åŠ± - å¥–åŠ±çš„æ ‡å‡†å·®**ã€‚**å¦‚æœä½ åœ¨æ’è¡Œæ¦œä¸Šçœ‹ä¸åˆ°ä½ çš„æ¨¡å‹ï¼Œè¯·å‰å¾€æ’è¡Œæ¦œé¡µé¢åº•éƒ¨å¹¶ç‚¹å‡»åˆ·æ–°æŒ‰é’®**ã€‚

**å¦‚æœæ‰¾ä¸åˆ°ä½ çš„æ¨¡å‹ï¼Œè¯·å‰å¾€é¡µé¢åº•éƒ¨å¹¶ç‚¹å‡»åˆ·æ–°æŒ‰é’®ã€‚**

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ† ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process

ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ä½ çš„è¿›åº¦ ğŸ‘‰ https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course


**è¦å¼€å§‹å®è·µç»ƒä¹ ï¼Œè¯·ç‚¹å‡»"åœ¨Colabä¸­æ‰“å¼€"æŒ‰é’®** ğŸ‘‡ï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit4/unit4.ipynb)

æˆ‘ä»¬å¼ºçƒˆ**å»ºè®®å­¦ç”Ÿä½¿ç”¨Google Colabè¿›è¡Œå®è·µç»ƒä¹ **ï¼Œè€Œä¸æ˜¯åœ¨ä¸ªäººç”µè„‘ä¸Šè¿è¡Œã€‚

é€šè¿‡ä½¿ç”¨Google Colabï¼Œ**ä½ å¯ä»¥ä¸“æ³¨äºå­¦ä¹ å’Œå®éªŒï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¾ç½®ç¯å¢ƒçš„æŠ€æœ¯æ–¹é¢**ã€‚

# ç¬¬4å•å…ƒï¼šä½¿ç”¨PyTorchç¼–å†™ä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceã€‚å¹¶æµ‹è¯•å…¶é²æ£’æ€§ ğŸ’ª

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png" alt="thumbnail"/>


åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œä½ å°†ä»å¤´å¼€å§‹ç¼–å†™ä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šReinforceï¼ˆä¹Ÿç§°ä¸ºè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰ã€‚

Reinforceæ˜¯ä¸€ç§*åŸºäºç­–ç•¥çš„æ–¹æ³•*ï¼šä¸€ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒ**å°è¯•ç›´æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œè€Œä¸ä½¿ç”¨åŠ¨ä½œå€¼å‡½æ•°**ã€‚

æ›´å‡†ç¡®åœ°è¯´ï¼ŒReinforceæ˜¯ä¸€ç§*ç­–ç•¥æ¢¯åº¦æ–¹æ³•*ï¼Œæ˜¯*åŸºäºç­–ç•¥çš„æ–¹æ³•*çš„ä¸€ä¸ªå­ç±»ï¼Œæ—¨åœ¨**é€šè¿‡ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ä¼°è®¡æœ€ä¼˜ç­–ç•¥çš„æƒé‡æ¥ç›´æ¥ä¼˜åŒ–ç­–ç•¥**ã€‚

ä¸ºäº†æµ‹è¯•å…¶é²æ£’æ€§ï¼Œæˆ‘ä»¬å°†åœ¨2ä¸ªä¸åŒçš„ç®€å•ç¯å¢ƒä¸­è®­ç»ƒå®ƒï¼š
- Cartpole-v1
- PixelcopterEnv

â¬‡ï¸ è¿™é‡Œæ˜¯**ä½ åœ¨æœ¬ç¬”è®°æœ¬ç»“æŸæ—¶å°†å®ç°çš„ç¤ºä¾‹**ã€‚ â¬‡ï¸

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>


### ğŸ® ç¯å¢ƒï¼š

- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)
- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)

### ğŸ“š RLåº“ï¼š

- Python
- PyTorch



æˆ‘ä»¬ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœä½ åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯·[åœ¨GitHubä»“åº“ä¸Šæå‡ºissue](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ ğŸ†

åœ¨ç¬”è®°æœ¬ç»“æŸæ—¶ï¼Œä½ å°†ï¼š

- èƒ½å¤Ÿ**ä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™Reinforceç®—æ³•**ã€‚
- èƒ½å¤Ÿ**ä½¿ç”¨ç®€å•ç¯å¢ƒæµ‹è¯•ä½ çš„æ™ºèƒ½ä½“çš„é²æ£’æ€§**ã€‚
- èƒ½å¤Ÿ**å°†ä½ è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“æ¨é€åˆ°Hub**ï¼Œé™„å¸¦ç²¾ç¾çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•° ğŸ”¥ã€‚

## å…ˆå†³æ¡ä»¶ ğŸ—ï¸

åœ¨æ·±å…¥ç¬”è®°æœ¬ä¹‹å‰ï¼Œä½ éœ€è¦ï¼š

ğŸ”² ğŸ“š [é€šè¿‡é˜…è¯»ç¬¬4å•å…ƒå­¦ä¹ ç­–ç•¥æ¢¯åº¦](https://huggingface.co/deep-rl-course/unit4/introduction)

# è®©æˆ‘ä»¬ä»å¤´å¼€å§‹ç¼–å†™Reinforceç®—æ³• ğŸ”¥

## ä¸€äº›å»ºè®® ğŸ’¡

æœ€å¥½åœ¨Google Driveä¸Šçš„å‰¯æœ¬ä¸­è¿è¡Œè¿™ä¸ªcolabï¼Œè¿™æ ·**å¦‚æœå®ƒè¶…æ—¶**ï¼Œä½ ä»ç„¶åœ¨Google Driveä¸Šä¿å­˜äº†ç¬”è®°æœ¬ï¼Œä¸éœ€è¦ä»å¤´å¼€å§‹å¡«å†™æ‰€æœ‰å†…å®¹ã€‚

è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥æŒ‰`Ctrl + S`æˆ–`æ–‡ä»¶ > åœ¨Google Driveä¸­ä¿å­˜å‰¯æœ¬`ã€‚

## è®¾ç½®GPU ğŸ’ª

- ä¸ºäº†**åŠ é€Ÿæ™ºèƒ½ä½“çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPU**ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¯·å‰å¾€`è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg" alt="GPU Step 1" />

- `ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg" alt="GPU Step 2" />

## åˆ›å»ºè™šæ‹Ÿæ˜¾ç¤ºå™¨ ğŸ–¥

åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå›æ”¾è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨colabï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥èƒ½å¤Ÿæ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

ä»¥ä¸‹å•å…ƒå°†å®‰è£…åº“å¹¶åˆ›å»ºå¹¶è¿è¡Œè™šæ‹Ÿå±å¹• ğŸ–¥

```python
%%capture
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip install pyvirtualdisplay
!pip install pyglet==1.5.1
```

```python
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## å®‰è£…ä¾èµ–é¡¹ ğŸ”½

ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ã€‚æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š

- `gym`
- `gym-games`ï¼šä½¿ç”¨PyGameåˆ¶ä½œçš„é¢å¤–gymç¯å¢ƒã€‚
- `huggingface_hub`ï¼šHubä½œä¸ºä¸€ä¸ªä¸­å¿ƒä½ç½®ï¼Œä»»ä½•äººéƒ½å¯ä»¥åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€æŒ‡æ ‡ã€å¯è§†åŒ–å’Œå…¶ä»–åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½å°†ä½¿ä½ èƒ½å¤Ÿè½»æ¾åœ°ä¸ä»–äººåä½œã€‚

ä½ å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬å®‰è£…gymè€Œä¸æ˜¯gymnasiumï¼ˆgymçš„æ›´æ–°ç‰ˆæœ¬ï¼‰ï¼Ÿ**å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„gym-gameså°šæœªæ›´æ–°ä¸ºæ”¯æŒgymnasium**ã€‚

ä½ åœ¨è¿™é‡Œä¼šé‡åˆ°çš„åŒºåˆ«ï¼š
- åœ¨`gym`ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰`terminated`å’Œ`truncated`ï¼Œåªæœ‰`done`ã€‚
- åœ¨`gym`ä¸­ï¼Œä½¿ç”¨`env.step()`è¿”å›`state, reward, done, info`

ä½ å¯ä»¥åœ¨è¿™é‡Œäº†è§£æ›´å¤šå…³äºGymå’ŒGymnasiumä¹‹é—´çš„åŒºåˆ« ğŸ‘‰ https://gymnasium.farama.org/content/migration-guide/

ä½ å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„Reinforceæ¨¡å‹ ğŸ‘‰ https://huggingface.co/models?other=reinforce

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æ‰€æœ‰æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ ğŸ‘‰ https://huggingface.co/models?pipeline_tag=reinforcement-learning


```bash
!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt
```

## å¯¼å…¥åŒ… ğŸ“¦

é™¤äº†å¯¼å…¥å·²å®‰è£…çš„åº“å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¼å…¥ï¼š

- `imageio`ï¼šä¸€ä¸ªå°†å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆå›æ”¾è§†é¢‘çš„åº“



```python
import numpy as np

from collections import deque

import matplotlib.pyplot as plt
%matplotlib inline

# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Gym
import gym
import gym_pygame

# Hugging Face Hub
from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.
import imageio
```

## æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦æœ‰GPU

- è®©æˆ‘ä»¬æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦æœ‰GPU
- å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œä½ åº”è¯¥çœ‹åˆ°`device:cuda0`

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

```python
print(device)
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å®ç°æˆ‘ä»¬çš„Reinforceç®—æ³• ğŸ”¥

# ç¬¬ä¸€ä¸ªæ™ºèƒ½ä½“ï¼šç©CartPole-v1 ğŸ¤–

## åˆ›å»ºCartPoleç¯å¢ƒå¹¶äº†è§£å®ƒçš„å·¥ä½œåŸç†

### [ç¯å¢ƒ ğŸ®](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)

### ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨åƒCartPole-v1è¿™æ ·çš„ç®€å•ç¯å¢ƒï¼Ÿ

æ­£å¦‚[å¼ºåŒ–å­¦ä¹ æŠ€å·§å’Œçªé—¨](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ä¸­æ‰€è§£é‡Šçš„ï¼Œå½“ä½ ä»å¤´å®ç°ä½ çš„æ™ºèƒ½ä½“æ—¶ï¼Œä½ éœ€è¦**ç¡®ä¿å®ƒæ­£å¸¸å·¥ä½œï¼Œå¹¶åœ¨ç®€å•ç¯å¢ƒä¸­æ‰¾åˆ°bugï¼Œç„¶åå†æ·±å…¥**ï¼Œå› ä¸ºåœ¨ç®€å•ç¯å¢ƒä¸­æ‰¾åˆ°bugä¼šå®¹æ˜“å¾—å¤šã€‚


> å°è¯•åœ¨ç©å…·é—®é¢˜ä¸Šæœ‰ä¸€äº›"ç”Ÿå‘½è¿¹è±¡"


> é€šè¿‡ä½¿å…¶åœ¨è¶Šæ¥è¶Šéš¾çš„ç¯å¢ƒä¸­è¿è¡Œæ¥éªŒè¯å®ç°ï¼ˆä½ å¯ä»¥å°†ç»“æœä¸RL zooè¿›è¡Œæ¯”è¾ƒï¼‰ã€‚å¯¹äºè¿™ä¸€æ­¥ï¼Œä½ é€šå¸¸éœ€è¦è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚


### CartPole-v1ç¯å¢ƒ

> ä¸€ä¸ªæ†é€šè¿‡ä¸€ä¸ªéé©±åŠ¨å…³èŠ‚è¿æ¥åˆ°ä¸€ä¸ªæ¨è½¦ä¸Šï¼Œè¯¥æ¨è½¦æ²¿ç€æ— æ‘©æ“¦çš„è½¨é“ç§»åŠ¨ã€‚é’Ÿæ‘†ç›´ç«‹æ”¾ç½®åœ¨æ¨è½¦ä¸Šï¼Œç›®æ ‡æ˜¯é€šè¿‡åœ¨æ¨è½¦ä¸Šæ–½åŠ å·¦å³æ–¹å‘çš„åŠ›æ¥å¹³è¡¡æ†ã€‚


æ‰€ä»¥ï¼Œæˆ‘ä»¬ä»CartPole-v1å¼€å§‹ã€‚ç›®æ ‡æ˜¯å‘å·¦æˆ–å‘å³æ¨åŠ¨æ¨è½¦ï¼Œ**ä½¿æ†ä¿æŒå¹³è¡¡**ã€‚

å¦‚æœå‡ºç°ä»¥ä¸‹æƒ…å†µï¼Œå›åˆç»“æŸï¼š
- æ†è§’åº¦å¤§äºÂ±12Â°
- æ¨è½¦ä½ç½®å¤§äºÂ±2.4
- å›åˆé•¿åº¦å¤§äº500

æ¯ä¸ªæ—¶é—´æ­¥æ†ä¿æŒå¹³è¡¡ï¼Œæˆ‘ä»¬éƒ½ä¼šè·å¾—+1çš„å¥–åŠ± ğŸ’°ã€‚

```python
env_id = "CartPole-v1"
# Create the env
env = gym.make(env_id)

# Create the evaluation env
eval_env = gym.make(env_id)

# Get the state space and action space
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

## è®©æˆ‘ä»¬æ„å»ºReinforceæ¶æ„

è¿™ä¸ªå®ç°åŸºäºä¸‰ä¸ªå®ç°ï¼š
- [PyTorchå®˜æ–¹å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)
- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)
- [Chris1nexuså¯¹é›†æˆçš„æ”¹è¿›](https://github.com/huggingface/deep-rl-class/pull/95)

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png" alt="Reinforce"/>

æ‰€ä»¥æˆ‘ä»¬éœ€è¦ï¼š
- ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼ˆfc1å’Œfc2ï¼‰ã€‚
- ä½¿ç”¨ReLUä½œä¸ºfc1çš„æ¿€æ´»å‡½æ•°
- ä½¿ç”¨Softmaxè¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Create two fully connected layers
       
        ...
    def forward(self, x):
    
    ...
    def act(self, state):
        """
        Given a state, take action
        """
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

### è§£å†³æ–¹æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = np.argmax(m)
        return action.item(), m.log_prob(action)
```

æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œä½ èƒ½çŒœå‡ºæ˜¯åœ¨å“ªé‡Œå—ï¼Ÿ

- è®©æˆ‘ä»¬è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥æ‰¾å‡ºé—®é¢˜ï¼š

```python
debug_policy = Policy(s_size, a_size, 64).to(device)
debug_policy.act(env.reset())
```

- è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°é”™è¯¯æç¤º`ValueError: The value argument to log_prob must be a Tensor`

- è¿™æ„å‘³ç€`m.log_prob(action)`ä¸­çš„`action`å¿…é¡»æ˜¯ä¸€ä¸ªå¼ é‡**ä½†å®ƒä¸æ˜¯ã€‚**

- ä½ çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿæ£€æŸ¥actå‡½æ•°å¹¶å°è¯•æ‰¾å‡ºä¸ºä»€ä¹ˆå®ƒä¸å·¥ä½œã€‚

å»ºè®® ğŸ’¡ï¼šè¿™ä¸ªå®ç°ä¸­æœ‰é—®é¢˜ã€‚è®°ä½ï¼Œå¯¹äºactå‡½æ•°ï¼Œ**æˆ‘ä»¬å¸Œæœ›ä»åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ**ã€‚


### ï¼ˆçœŸæ­£çš„ï¼‰è§£å†³æ–¹æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

é€šè¿‡ä½¿ç”¨CartPoleï¼Œè°ƒè¯•å˜å¾—æ›´å®¹æ˜“ï¼Œå› ä¸º**æˆ‘ä»¬çŸ¥é“é”™è¯¯æ¥è‡ªæˆ‘ä»¬çš„é›†æˆï¼Œè€Œä¸æ˜¯æ¥è‡ªæˆ‘ä»¬çš„ç®€å•ç¯å¢ƒ**ã€‚

- ç”±äº**æˆ‘ä»¬æƒ³è¦ä»åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ**ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨`action = np.argmax(m)`ï¼Œå› ä¸ºå®ƒæ€»æ˜¯è¾“å‡ºå…·æœ‰æœ€é«˜æ¦‚ç‡çš„åŠ¨ä½œã€‚

- æˆ‘ä»¬éœ€è¦å°†å…¶æ›¿æ¢ä¸º`action = m.sample()`ï¼Œå®ƒå°†ä»æ¦‚ç‡åˆ†å¸ƒP(.|s)ä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ

### è®©æˆ‘ä»¬æ„å»ºReinforceè®­ç»ƒç®—æ³•
è¿™æ˜¯Reinforceç®—æ³•çš„ä¼ªä»£ç ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png" alt="Policy gradient pseudocode"/>



- å½“æˆ‘ä»¬è®¡ç®—å›æŠ¥Gtï¼ˆç¬¬6è¡Œï¼‰æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬è®¡ç®—çš„æ˜¯**ä»æ—¶é—´æ­¥tå¼€å§‹**çš„æŠ˜æ‰£å¥–åŠ±æ€»å’Œã€‚

- ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæˆ‘ä»¬çš„ç­–ç•¥åº”è¯¥åª**æ ¹æ®åæœæ¥å¼ºåŒ–åŠ¨ä½œ**ï¼šæ‰€ä»¥åœ¨é‡‡å–åŠ¨ä½œä¹‹å‰è·å¾—çš„å¥–åŠ±æ˜¯æ— ç”¨çš„ï¼ˆå› ä¸ºå®ƒä»¬ä¸æ˜¯ç”±äºè¯¥åŠ¨ä½œé€ æˆçš„ï¼‰ï¼Œ**åªæœ‰åœ¨åŠ¨ä½œä¹‹åè·å¾—çš„å¥–åŠ±æ‰é‡è¦**ã€‚

- åœ¨ç¼–å†™ä»£ç ä¹‹å‰ï¼Œä½ åº”è¯¥é˜…è¯»[ä¸è¦è®©è¿‡å»åˆ†æ•£ä½ çš„æ³¨æ„åŠ›](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)è¿™ä¸€éƒ¨åˆ†ï¼Œå®ƒè§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨reward-to-goç­–ç•¥æ¢¯åº¦ã€‚

æˆ‘ä»¬ä½¿ç”¨[Chris1nexus](https://github.com/Chris1nexus)ç¼–å†™çš„ä¸€ç§æœ‰è¶£æŠ€æœ¯æ¥**é«˜æ•ˆè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥**ã€‚æ³¨é‡Šè§£é‡Šäº†è¿™ä¸ªè¿‡ç¨‹ã€‚ä¹Ÿä¸è¦çŠ¹è±«[æŸ¥çœ‹PRè§£é‡Š](https://github.com/huggingface/deep-rl-class/pull/95)
ä½†æ€»ä½“æ€è·¯æ˜¯**é«˜æ•ˆè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥**ã€‚

ä½ å¯èƒ½ä¼šé—®çš„ç¬¬äºŒä¸ªé—®é¢˜æ˜¯**ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦æœ€å°åŒ–æŸå¤±**ï¼Ÿæˆ‘ä»¬ä¹‹å‰ä¸æ˜¯è®¨è®ºè¿‡æ¢¯åº¦ä¸Šå‡è€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™å—ï¼Ÿ

- æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–æˆ‘ä»¬çš„æ•ˆç”¨å‡½æ•°$J(\theta)$ï¼Œä½†åœ¨PyTorchå’ŒTensorFlowä¸­ï¼Œ**æœ€å°åŒ–ç›®æ ‡å‡½æ•°**æ›´å¥½ã€‚
    - æ‰€ä»¥å‡è®¾æˆ‘ä»¬æƒ³åœ¨æŸä¸ªæ—¶é—´æ­¥å¼ºåŒ–åŠ¨ä½œ3ã€‚åœ¨è®­ç»ƒä¹‹å‰ï¼Œè¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡Pæ˜¯0.25ã€‚
    - æ‰€ä»¥æˆ‘ä»¬æƒ³ä¿®æ”¹\\(theta\\)ä½¿å¾—\\(\pi_\theta(a_3|s; \theta) > 0.25\\)
    - å› ä¸ºæ‰€æœ‰Pçš„æ€»å’Œå¿…é¡»ä¸º1ï¼Œæœ€å¤§åŒ–\\(pi_\theta(a_3|s; \theta)\\)å°†**æœ€å°åŒ–å…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡**ã€‚
    - æ‰€ä»¥æˆ‘ä»¬åº”è¯¥å‘Šè¯‰PyTorch**æœ€å°åŒ–\\(1 - \pi_\theta(a_3|s; \theta)\\)**ã€‚
    - å½“\\(\pi_\theta(a_3|s; \theta)\\)æ¥è¿‘1æ—¶ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°æ¥è¿‘0ã€‚
    - æ‰€ä»¥æˆ‘ä»¬é¼“åŠ±æ¢¯åº¦æœ€å¤§åŒ–\\(\pi_\theta(a_3|s; \theta)\\)


```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes+1):
        saved_log_probs = []
        rewards = []
        state = # TODO: reset the environment
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = # TODO get the action
            saved_log_probs.append(log_prob)
            state, reward, done, _ = # TODO: take an env step
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t

        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...


        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = (returns[0] if len(returns)>0 else 0)
            returns.appendleft(    ) # TODO: complete here

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()

        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))

    return scores
```

#### è§£å†³æ–¹æ¡ˆ

```python
def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):
    # Help us to calculate the score during the training
    scores_deque = deque(maxlen=100)
    scores = []
    # Line 3 of pseudocode
    for i_episode in range(1, n_training_episodes + 1):
        saved_log_probs = []
        rewards = []
        state = env.reset()
        # Line 4 of pseudocode
        for t in range(max_t):
            action, log_prob = policy.act(state)
            saved_log_probs.append(log_prob)
            state, reward, done, _ = env.step(action)
            rewards.append(reward)
            if done:
                break
        scores_deque.append(sum(rewards))
        scores.append(sum(rewards))

        # Line 6 of pseudocode: calculate the return
        returns = deque(maxlen=max_t)
        n_steps = len(rewards)
        # Compute the discounted returns at each timestep,
        # as
        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t
        #
        # In O(N) time, where N is the number of time steps
        # (this definition of the discounted return G_t follows the definition of this quantity
        # shown at page 44 of Sutton&Barto 2017 2nd draft)
        # G_t = r_(t+1) + r_(t+2) + ...

        # Given this formulation, the returns at each timestep t can be computed
        # by re-using the computed future returns G_(t+1) to compute the current return G_t
        # G_t = r_(t+1) + gamma*G_(t+1)
        # G_(t-1) = r_t + gamma* G_t
        # (this follows a dynamic programming approach, with which we memorize solutions in order
        # to avoid computing them multiple times)

        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)
        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...

        ## Given the above, we calculate the returns at timestep t as:
        #               gamma[t] * return[t] + reward[t]
        #
        ## We compute this starting from the last timestep to the first, in order
        ## to employ the formula presented above and avoid redundant computations that would be needed
        ## if we were to do it from first to last.

        ## Hence, the queue "returns" will hold the returns in chronological order, from t=0 to t=n_steps
        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)
        ## a normal python list would instead require O(N) to do this.
        for t in range(n_steps)[::-1]:
            disc_return_t = returns[0] if len(returns) > 0 else 0
            returns.appendleft(gamma * disc_return_t + rewards[t])

        ## standardization of the returns is employed to make training more stable
        eps = np.finfo(np.float32).eps.item()
        ## eps is the smallest representable float, which is
        # added to the standard deviation of the returns to avoid numerical instabilities
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + eps)

        # Line 7:
        policy_loss = []
        for log_prob, disc_return in zip(saved_log_probs, returns):
            policy_loss.append(-log_prob * disc_return)
        policy_loss = torch.cat(policy_loss).sum()

        # Line 8: PyTorch prefers gradient descent
        optimizer.zero_grad()
        policy_loss.backward()
        optimizer.step()

        if i_episode % print_every == 0:
            print("Episode {}\tAverage Score: {:.2f}".format(i_episode, np.mean(scores_deque)))

    return scores
```

##  è®­ç»ƒå®ƒ
- æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„æ™ºèƒ½ä½“ã€‚
- ä½†é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°çš„å˜é‡ã€‚
- ä½ å¯ä»¥ï¼ˆè€Œä¸”åº”è¯¥ğŸ˜‰ï¼‰æ›´æ”¹è®­ç»ƒå‚æ•°

```python
cartpole_hyperparameters = {
    "h_size": 16,
    "n_training_episodes": 1000,
    "n_evaluation_episodes": 10,
    "max_t": 1000,
    "gamma": 1.0,
    "lr": 1e-2,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

```python
# Create policy and place it to the device
cartpole_policy = Policy(
    cartpole_hyperparameters["state_space"],
    cartpole_hyperparameters["action_space"],
    cartpole_hyperparameters["h_size"],
).to(device)
cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters["lr"])
```

```python
scores = reinforce(
    cartpole_policy,
    cartpole_optimizer,
    cartpole_hyperparameters["n_training_episodes"],
    cartpole_hyperparameters["max_t"],
    cartpole_hyperparameters["gamma"],
    100,
)
```

## å®šä¹‰è¯„ä¼°æ–¹æ³• ğŸ“
- è¿™é‡Œæˆ‘ä»¬å®šä¹‰è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºæµ‹è¯•æˆ‘ä»¬çš„Reinforceæ™ºèƒ½ä½“ã€‚

```python
def evaluate_agent(env, max_steps, n_eval_episodes, policy):
    """
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param policy: The Reinforce agent
    """
    episode_rewards = []
    for episode in range(n_eval_episodes):
        state = env.reset()
        step = 0
        done = False
        total_rewards_ep = 0

        for step in range(max_steps):
            action, _ = policy.act(state)
            new_state, reward, done, info = env.step(action)
            total_rewards_ep += reward

            if done:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## è¯„ä¼°æˆ‘ä»¬çš„æ™ºèƒ½ä½“ ğŸ“ˆ

```python
evaluate_agent(
    eval_env, cartpole_hyperparameters["max_t"], cartpole_hyperparameters["n_evaluation_episodes"], cartpole_policy
)
```

### åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ ğŸ”¥
ç°åœ¨æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒåå–å¾—äº†ä¸é”™çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ¤—ã€‚

è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å¡ç‰‡çš„ä¾‹å­ï¼š

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png"/>

### æ¨é€åˆ°Hub
#### è¯·å‹¿ä¿®æ”¹æ­¤ä»£ç 

```python
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
import imageio

import tempfile

import os
```

```python
def record_video(env, policy, out_directory, fps=30):
    """
    Generate a replay video of the agent
    :param env
    :param Qtable: Qtable of our agent
    :param out_directory
    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
    """
    images = []
    done = False
    state = env.reset()
    img = env.render(mode="rgb_array")
    images.append(img)
    while not done:
        # Take the action (index) that have the maximum expected future reward given that state
        action, _ = policy.act(state)
        state, reward, done, info = env.step(action)  # We directly put next_state = state for recording logic
        img = env.render(mode="rgb_array")
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```python
def push_to_hub(repo_id,
                model,
                hyperparameters,
                eval_env,
                video_fps=30
                ):
  """
  Evaluate, Generate a video and Upload a model to Hugging Face Hub.
  This method does the complete pipeline:
  - It evaluates the model
  - It generates the model card
  - It generates a replay video of the agent
  - It pushes everything to the Hub

  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
  :param model: the pytorch model we want to save
  :param hyperparameters: training hyperparameters
  :param eval_env: evaluation environment
  :param video_fps: how many frame per seconds to record our video replay
  """

  _, repo_name = repo_id.split("/")
  api = HfApi()

  # Step 1: Create the repo
  repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
  )

  with tempfile.TemporaryDirectory() as tmpdirname:
    local_directory = Path(tmpdirname)

    # Step 2: Save the model
    torch.save(model, local_directory / "model.pt")

    # Step 3: Save the hyperparameters to JSON
    with open(local_directory / "hyperparameters.json", "w") as outfile:
      json.dump(hyperparameters, outfile)

    # Step 4: Evaluate the model and build JSON
    mean_reward, std_reward = evaluate_agent(eval_env,
                                            hyperparameters["max_t"],
                                            hyperparameters["n_evaluation_episodes"],
                                            model)
    # Get datetime
    eval_datetime = datetime.datetime.now()
    eval_form_datetime = eval_datetime.isoformat()

    evaluate_data = {
          "env_id": hyperparameters["env_id"],
          "mean_reward": mean_reward,
          "n_evaluation_episodes": hyperparameters["n_evaluation_episodes"],
          "eval_datetime": eval_form_datetime,
    }

    # Write a JSON file
    with open(local_directory / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = hyperparameters["env_id"]

    metadata = {}
    metadata["tags"] = [
          env_name,
          "reinforce",
          "reinforcement-learning",
          "custom-implementation",
          "deep-rl-class"
      ]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
      )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Reinforce** Agent playing **{env_id}**
  This is a trained model of a **Reinforce** agent playing **{env_id}** .
  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction
  """

    readme_path = local_directory / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
          readme = f.read()
    else:
      readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
      f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path =  local_directory / "replay.mp4"
    record_video(env, model, video_path, video_fps)

    # Step 7. Push everything to the Hub
    api.upload_folder(
          repo_id=repo_id,
          folder_path=local_directory,
          path_in_repo=".",
    )

    print(f"Your model is pushed to the Hub. You can view your model here: {repo_url}")
```

é€šè¿‡ä½¿ç”¨`push_to_hub`ï¼Œ**ä½ å¯ä»¥è¯„ä¼°ã€è®°å½•å›æ”¾ã€ç”Ÿæˆæ™ºèƒ½ä½“çš„æ¨¡å‹å¡ç‰‡ï¼Œå¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚

è¿™æ ·ï¼š
- ä½ å¯ä»¥**å±•ç¤ºä½ çš„å·¥ä½œ** ğŸ”¥
- ä½ å¯ä»¥**å¯è§†åŒ–ä½ çš„æ™ºèƒ½ä½“çš„æ¸¸æˆè¿‡ç¨‹** ğŸ‘€
- ä½ å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«ä¸€ä¸ªå…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„æ™ºèƒ½ä½“** ğŸ’¾
- ä½ å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹ä½ çš„æ™ºèƒ½ä½“ä¸åŒå­¦ç›¸æ¯”è¡¨ç°å¦‚ä½•** ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard


è¦èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ï¼Œè¿˜éœ€è¦éµå¾ªä¸‰ä¸ªæ­¥éª¤ï¼š

1ï¸âƒ£ ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ªHFè´¦æˆ· â¡ https://huggingface.co/join

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åä½ éœ€è¦ä»Hugging Faceç½‘ç«™å­˜å‚¨ä½ çš„è®¤è¯ä»¤ç‰Œã€‚
- åˆ›å»ºä¸€ä¸ªæ–°ä»¤ç‰Œï¼ˆhttps://huggingface.co/settings/tokensï¼‰**å…·æœ‰å†™å…¥æƒé™**


<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg" alt="Create HF Token" />



```python
notebook_login()
```

å¦‚æœä½ ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œä½ éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰

3ï¸âƒ£ æˆ‘ä»¬ç°åœ¨å‡†å¤‡ä½¿ç”¨`package_to_hub()`å‡½æ•°å°†è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥

```python
repo_id = ""  # TODO å®šä¹‰ä½ çš„ä»“åº“ID {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    cartpole_policy,  # æˆ‘ä»¬æƒ³è¦ä¿å­˜çš„æ¨¡å‹
    cartpole_hyperparameters,  # è¶…å‚æ•°
    eval_env,  # è¯„ä¼°ç¯å¢ƒ
    video_fps=30
)
```

ç°åœ¨æˆ‘ä»¬å·²ç»æµ‹è¯•äº†æˆ‘ä»¬å®ç°çš„é²æ£’æ€§ï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´å¤æ‚çš„ç¯å¢ƒï¼šPixelCopter ğŸš




## ç¬¬äºŒä¸ªæ™ºèƒ½ä½“ï¼šPixelCopter ğŸš

### ç ”ç©¶PixelCopterç¯å¢ƒ ğŸ‘€
- [ç¯å¢ƒæ–‡æ¡£](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)


```python
env_id = "Pixelcopter-PLE-v0"
env = gym.make(env_id)
eval_env = gym.make(env_id)
s_size = env.observation_space.shape[0]
a_size = env.action_space.n
```

```python
print("_____OBSERVATION SPACE_____ \n")
print("The State Space is: ", s_size)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

```python
print("\n _____ACTION SPACE_____ \n")
print("The Action Space is: ", a_size)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

è§‚å¯Ÿç©ºé—´(7) ğŸ‘€:
- ç©å®¶yä½ç½®
- ç©å®¶é€Ÿåº¦
- ç©å®¶åˆ°åœ°é¢çš„è·ç¦»
- ç©å®¶åˆ°å¤©èŠ±æ¿çš„è·ç¦»
- ä¸‹ä¸€ä¸ªéšœç¢ç‰©ä¸ç©å®¶çš„xè·ç¦»
- ä¸‹ä¸€ä¸ªéšœç¢ç‰©é¡¶éƒ¨yä½ç½®
- ä¸‹ä¸€ä¸ªéšœç¢ç‰©åº•éƒ¨yä½ç½®

åŠ¨ä½œç©ºé—´(2) ğŸ®:
- å‘ä¸Šï¼ˆæŒ‰åŠ é€Ÿå™¨ï¼‰
- ä»€ä¹ˆéƒ½ä¸åšï¼ˆä¸æŒ‰åŠ é€Ÿå™¨ï¼‰

å¥–åŠ±å‡½æ•° ğŸ’°:
- æ¯é€šè¿‡ä¸€ä¸ªå‚ç›´éšœç¢ç‰©ï¼Œè·å¾—+1çš„æ­£å¥–åŠ±ã€‚æ¯æ¬¡è¾¾åˆ°ç»ˆæ­¢çŠ¶æ€æ—¶ï¼Œè·å¾—-1çš„è´Ÿå¥–åŠ±ã€‚

### å®šä¹‰æ–°çš„ç­–ç•¥ ğŸ§ 
- ç”±äºç¯å¢ƒæ›´å¤æ‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æ·±çš„ç¥ç»ç½‘ç»œ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Define the three layers here

    def forward(self, x):
        # Define the forward process here
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

#### è§£å†³æ–¹æ¡ˆ

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(s_size, h_size)
        self.fc2 = nn.Linear(h_size, h_size * 2)
        self.fc3 = nn.Linear(h_size * 2, a_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=1)

    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0).to(device)
        probs = self.forward(state).cpu()
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)
```

### å®šä¹‰è¶…å‚æ•° âš™ï¸
- å› ä¸ºè¿™ä¸ªç¯å¢ƒæ›´å¤æ‚ã€‚
- ç‰¹åˆ«æ˜¯éšè—å±‚å¤§å°ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ç¥ç»å…ƒã€‚

```python
pixelcopter_hyperparameters = {
    "h_size": 64,
    "n_training_episodes": 50000,
    "n_evaluation_episodes": 10,
    "max_t": 10000,
    "gamma": 0.99,
    "lr": 1e-4,
    "env_id": env_id,
    "state_space": s_size,
    "action_space": a_size,
}
```

###  è®­ç»ƒå®ƒ
- æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒæˆ‘ä»¬çš„æ™ºèƒ½ä½“ ğŸ”¥ã€‚

```python
# Create policy and place it to the device
# torch.manual_seed(50)
pixelcopter_policy = Policy(
    pixelcopter_hyperparameters["state_space"],
    pixelcopter_hyperparameters["action_space"],
    pixelcopter_hyperparameters["h_size"],
).to(device)
pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters["lr"])
```

```python
scores = reinforce(
    pixelcopter_policy,
    pixelcopter_optimizer,
    pixelcopter_hyperparameters["n_training_episodes"],
    pixelcopter_hyperparameters["max_t"],
    pixelcopter_hyperparameters["gamma"],
    1000,
)
```

### åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ ğŸ”¥

```python
repo_id = ""  # TODO å®šä¹‰ä½ çš„ä»“åº“ID {username/Reinforce-{model-id}}
push_to_hub(
    repo_id,
    pixelcopter_policy,  # æˆ‘ä»¬æƒ³è¦ä¿å­˜çš„æ¨¡å‹
    pixelcopter_hyperparameters,  # è¶…å‚æ•°
    eval_env,  # è¯„ä¼°ç¯å¢ƒ
    video_fps=30
)
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ ğŸ†

å­¦ä¹ çš„æœ€ä½³æ–¹å¼**æ˜¯è‡ªå·±å°è¯•**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„æ™ºèƒ½ä½“è¡¨ç°ä¸æ˜¯å¾ˆå¥½ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œä½ å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚ä½†ä¹Ÿå°è¯•æ‰¾åˆ°æ›´å¥½çš„å‚æ•°ã€‚

åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°ä½ çš„æ™ºèƒ½ä½“ã€‚ä½ èƒ½ç™»ä¸Šæ¦œé¦–å—ï¼Ÿ

ä»¥ä¸‹æ˜¯ä¸€äº›æ”€ç™»æ’è¡Œæ¦œçš„æƒ³æ³•ï¼š
* è®­ç»ƒæ›´å¤šæ­¥éª¤
* é€šè¿‡æŸ¥çœ‹ä½ çš„åŒå­¦ä»¬æ‰€åšçš„å·¥ä½œï¼Œå°è¯•ä¸åŒçš„è¶…å‚æ•° ğŸ‘‰ https://huggingface.co/models?other=reinforce
* **å°†ä½ æ–°è®­ç»ƒçš„æ¨¡å‹**æ¨é€åˆ°Hub ğŸ”¥
* **æ”¹è¿›å®ç°ä»¥é€‚åº”æ›´å¤æ‚çš„ç¯å¢ƒ**ï¼ˆä¾‹å¦‚ï¼Œå¦‚ä½•å°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼Ÿï¼‰

________________________________________________________________________

**æ­å–œä½ å®Œæˆæœ¬å•å…ƒ**ï¼è¿™é‡Œæœ‰å¾ˆå¤šä¿¡æ¯ã€‚
ä¹Ÿæ­å–œä½ å®Œæˆæ•™ç¨‹ã€‚ä½ åˆšåˆšä½¿ç”¨PyTorchä»å¤´ç¼–å†™äº†ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œå¹¶å°†å…¶åˆ†äº«åˆ°Hubä¸Š ğŸ¥³ã€‚

ä¸è¦çŠ¹è±«ï¼Œ**é€šè¿‡æ”¹è¿›æ›´å¤æ‚ç¯å¢ƒçš„å®ç°**æ¥è¿­ä»£æœ¬å•å…ƒï¼ˆä¾‹å¦‚ï¼Œå¦‚ä½•å°†ç½‘ç»œæ›´æ”¹ä¸ºå·ç§¯ç¥ç»ç½‘ç»œä»¥å¤„ç†å¸§ä½œä¸ºè§‚å¯Ÿï¼Ÿï¼‰ã€‚

åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­ï¼Œ**æˆ‘ä»¬å°†é€šè¿‡åœ¨Unityç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“æ¥å­¦ä¹ æ›´å¤šå…³äºUnity MLAgentsçš„çŸ¥è¯†**ã€‚è¿™æ ·ï¼Œä½ å°†å‡†å¤‡å¥½å‚åŠ **AIå¯¹æŠ—AIæŒ‘æˆ˜ï¼Œåœ¨é‚£é‡Œä½ å°†è®­ç»ƒä½ çš„æ™ºèƒ½ä½“ä¸å…¶ä»–æ™ºèƒ½ä½“åœ¨é›ªçƒå¤§æˆ˜å’Œè¶³çƒæ¯”èµ›ä¸­ç«äº‰**ã€‚

å¬èµ·æ¥å¾ˆæœ‰è¶£å§ï¼Ÿä¸‹æ¬¡è§ï¼

æœ€åï¼Œæˆ‘ä»¬å¾ˆæƒ³**å¬å¬ä½ å¯¹è¯¾ç¨‹çš„çœ‹æ³•ä»¥åŠæˆ‘ä»¬å¦‚ä½•æ”¹è¿›å®ƒ**ã€‚å¦‚æœä½ æœ‰ä¸€äº›åé¦ˆï¼Œè¯· ğŸ‘‰ [å¡«å†™è¿™ä¸ªè¡¨å•](https://forms.gle/BzKXWzLAGZESGNaE9)

æˆ‘ä»¬åœ¨ç¬¬5å•å…ƒè§ï¼ï¿½ï¿½

### æŒç»­å­¦ä¹ ï¼Œä¿æŒç²¾å½© ğŸ¤—
