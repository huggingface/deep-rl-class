# 什么是基于策略的方法？

强化学习的主要目标是**找到最优策略 \\(\pi^{*}\\)，使期望累积奖励最大化**。
因为强化学习基于*奖励假设*：**所有目标都可以描述为期望累积奖励的最大化。**

例如，在足球比赛中（你将在两个单元中训练智能体），目标是赢得比赛。我们可以在强化学习中将这个目标描述为
**最大化进入对手球门的进球数**（当球越过球门线时）。以及**最小化进入你自己球门的进球数**。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg" alt="Soccer" />

## 基于价值、基于策略和演员-评论家方法

在第一单元中，我们看到了两种方法来找到（或者，大多数时候，近似）这个最优策略 \\(\pi^{*}\\)。

- 在*基于价值的方法*中，我们学习一个价值函数。
  - 其思想是最优价值函数导致最优策略 \\(\pi^{*}\\)。
  - 我们的目标是**最小化预测值和目标值之间的损失**，以近似真实的动作价值函数。
  - 我们有一个策略，但它是隐式的，因为它**直接从价值函数生成**。例如，在Q学习中，我们使用了（epsilon-）贪婪策略。

- 另一方面，在*基于策略的方法*中，我们直接学习近似 \\(\pi^{*}\\)，而不需要学习价值函数。
  - 其思想是**参数化策略**。例如，使用神经网络 \\(\pi_\theta\\)，这个策略将输出动作的概率分布（随机策略）。
  - <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png" alt="stochastic policy" />
  - 我们的目标是**使用梯度上升最大化参数化策略的性能**。
  - 为此，我们控制参数 \\(\theta\\)，它将影响状态上动作的分布。

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png" alt="Policy based" />

- 下次，我们将学习*演员-评论家*方法，它是基于价值和基于策略方法的结合。

因此，通过基于策略的方法，我们可以直接优化我们的策略 \\(\pi_\theta\\)，使其输出动作的概率分布 \\(\pi_\theta(a|s)\\)，从而获得最佳累积回报。
为此，我们定义一个目标函数 \\(J(\theta)\\)，即期望累积奖励，我们**希望找到使这个目标函数最大化的值 \\(\theta\\)**。

## 基于策略和策略梯度方法的区别

策略梯度方法，我们将在本单元中学习，是基于策略方法的一个子类。在基于策略的方法中，优化大多数时候是*在线的*，因为对于每次更新，我们只使用**由我们最新版本的** \\(\pi_\theta\\) **收集的数据（轨迹）**。

这两种方法之间的区别**在于我们如何优化参数** \\(\theta\\)：

- 在*基于策略的方法*中，我们直接搜索最优策略。我们可以通过使用诸如爬山、模拟退火或进化策略等技术最大化目标函数的局部近似，**间接**优化参数 \\(\theta\\)。
- 在*策略梯度方法*中，因为它是基于策略方法的一个子类，我们直接搜索最优策略。但我们通过对目标函数 \\(J(\theta)\\) 的性能执行梯度上升，**直接**优化参数 \\(\theta\\)。

在深入了解策略梯度方法如何工作（目标函数、策略梯度定理、梯度上升等）之前，让我们研究基于策略方法的优势和劣势。
