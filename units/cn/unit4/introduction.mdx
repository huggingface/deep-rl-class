# 介绍 [[introduction]]

  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png" alt="thumbnail"/>

在上一单元中，我们学习了深度Q学习。在这种基于价值的深度强化学习算法中，我们**使用深度神经网络来近似状态下每个可能动作的不同Q值。**

自课程开始以来，我们只研究了基于价值的方法，**在这些方法中，我们估计价值函数作为寻找最优策略的中间步骤。**

<img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg" alt="Link value policy" />

在基于价值的方法中，策略 **\\(π\\) 仅仅因为动作价值估计而存在，因为策略只是一个函数**（例如，贪婪策略），它将选择给定状态下具有最高价值的动作。

通过基于策略的方法，我们希望直接优化策略，**而不需要学习价值函数这一中间步骤。**

所以今天，**我们将学习基于策略的方法，并研究这些方法的一个子集，称为策略梯度**。然后，我们将使用PyTorch从头实现我们的第一个策略梯度算法，称为蒙特卡洛 **Reinforce**。
然后，我们将使用CartPole-v1和PixelCopter环境测试其稳健性。

之后，你将能够迭代并改进这个实现，以适用于更高级的环境。

<figure class="image table text-center m-0 w-full">
  <img src="https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif" alt="Environments"/>
</figure>

让我们开始吧！
